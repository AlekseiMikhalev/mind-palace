{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This is my personal Mind Palace, where I collect knowledge from my learning journey. Every time I read some tech book, documentation, science article, or \"how to\" recipe, I make a summary in the respective section. After I watched a course or project I make a summary too. I took this approach to solve one simple problem - remember what I've read or learnt and be able to apply the knowledge in my life. Mind Palace Structure: Mind Palace consist of the following nested categories: Knowledge section (the widest knowledge group) Knowledge concept (category of the group related to the particular technology) Code snippets (examples of how to implement the particular knowledge concept) Code snippets also include the detailed explanation of each line of code. That helps me to understand how the code works and remember it later during the implementation. The knowledge base in this digital Mind Palace is related to various software development areas. My learning path is focused on expanding my knowledge step-by-step and eventually building a solid base for developing robust complex systems. Therefore, it includes knowledge from frontend and backend development, machine learning, data science, databases, software architecture, etc. Couple of words about me I am a self-taught software developer. I started my programming journey by heart. In my childhood I wanted to be a programmer. I took some gadgets (whether it was my grandfather's watch or music player from the town dump), disassemble them and tried to assemble again. I was extremely interested to understand how they work. But something went wrong and I became a lawyer. My lawyer career includes eight years in international law firms and venture funds as a corporate lawyer. Spending long hours on drafting documents, doing due diligence or revising prospectuses for capital market deals, I realised that it's not what I want to do for life. It is just not my passion. And in my spare time I started to learn programing. Step-by-step I switched into software engineering and left my lawyer career behind. That was, and actually still very hard... But you know what? I am happy! It gives me energy and meaningful start every day. And I won't give up Comments, recommendation and revisions I would be happy if you give me any useful comments, recommendation or point out on my mistakes or code errors.","title":"Introduction"},{"location":"#introduction","text":"This is my personal Mind Palace, where I collect knowledge from my learning journey. Every time I read some tech book, documentation, science article, or \"how to\" recipe, I make a summary in the respective section. After I watched a course or project I make a summary too. I took this approach to solve one simple problem - remember what I've read or learnt and be able to apply the knowledge in my life.","title":"Introduction"},{"location":"#mind-palace-structure","text":"Mind Palace consist of the following nested categories: Knowledge section (the widest knowledge group) Knowledge concept (category of the group related to the particular technology) Code snippets (examples of how to implement the particular knowledge concept) Code snippets also include the detailed explanation of each line of code. That helps me to understand how the code works and remember it later during the implementation. The knowledge base in this digital Mind Palace is related to various software development areas. My learning path is focused on expanding my knowledge step-by-step and eventually building a solid base for developing robust complex systems. Therefore, it includes knowledge from frontend and backend development, machine learning, data science, databases, software architecture, etc.","title":"Mind Palace Structure:"},{"location":"#couple-of-words-about-me","text":"I am a self-taught software developer. I started my programming journey by heart. In my childhood I wanted to be a programmer. I took some gadgets (whether it was my grandfather's watch or music player from the town dump), disassemble them and tried to assemble again. I was extremely interested to understand how they work. But something went wrong and I became a lawyer. My lawyer career includes eight years in international law firms and venture funds as a corporate lawyer. Spending long hours on drafting documents, doing due diligence or revising prospectuses for capital market deals, I realised that it's not what I want to do for life. It is just not my passion. And in my spare time I started to learn programing. Step-by-step I switched into software engineering and left my lawyer career behind. That was, and actually still very hard... But you know what? I am happy! It gives me energy and meaningful start every day. And I won't give up","title":"Couple of words about me"},{"location":"#comments-recommendation-and-revisions","text":"I would be happy if you give me any useful comments, recommendation or point out on my mistakes or code errors.","title":"Comments, recommendation and revisions"},{"location":"backend/","text":"General Knowledge about Backend This section contains information about general backend knowledge. More specific knowledge are collected into the more narrowed sections.","title":"Backend"},{"location":"backend/#general-knowledge-about-backend","text":"This section contains information about general backend knowledge. More specific knowledge are collected into the more narrowed sections.","title":"General Knowledge about Backend"},{"location":"backend/concurrency_parallelism/","text":"This section contains knowledge about asynchronous programming in Python Sources : Books: Advanced Python Programming Asynchronous programming is a way of dealing with slow and unpredictable resources. Rather than waiting idly for resources to become available, asynchronous programs can handle multiple resources concurrently and efficiently. Programming in an asynchronous way can be challenging because it is necessary to deal with external requests that can arrive in any order, may take a variable amount of time, or may fail unpredictably. Concurrency is a way to implement a system that can deal with multiple requests at the same time. The idea is that we can move on and start handling other resources while we wait for a resource to become available. Concurrency works by splitting a task into smaller subtasks that can be executed out of order so that multiple tasks can be partially advanced without waiting for the previous tasks to finish. import threading def wait_and_print_async ( msg ): def callback (): print ( msg ) timer = threading . Timer ( 1.0 , callback ) #(1) timer . start () a callback is simply a function that will be called when the timer expires. The strategy used by threading.Timer involves starting a new thread that can execute code in parallel. Another library for concurrent code and reative programming is RxPy Another idea to improve efficiency is to use multiprocessing. [TO BE CONTINUED...]","title":"Concurrency and Parallelism"},{"location":"backend/efficient_programming/","text":"Efficient Programming in Python Sources : Books: Advanced Python Programming Profiling Profilers in Python Profiling - technique that allows to pinpoint the most resource-intensive parts of an application. Profiler - program that runs an aplication and monitors how long each function takes to execute, so it detects the functions on which our application spends of its time. pytest-benchmark is used for testing running time of specific functions. See documentation of pytest-benchmark for details. Another profiling tool is cProfile . from simul import benchmark import cProfile pr = cProfile . Profile () pr . enable () benchmark () pr . disable () pr . print_stats () KCachegrind - graphical user interface visualizing profiling output. How to do profiling: Check modules of the app with cProfile or other profiler in order to understand which module is time-consuming Use line_profiler to check functions line by line. See the documentation Profiling memory usage Memory profiler summarizes the information of memory usage of process. Optimizing code Methods of optimzation: improve algorithms used minimize the number of instructions Using the right data structures Estimation time of operations per data sctructure: list of 10,000 size deque (double-ended queue) of 10,000 size Tip A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the bisect module . bisect allows fast search on sorted arrays. insert bisect collection = [ 1 , 2 , 4 , 5 , 6 ] bisect . bisect ( collection , 3 ) # Result: 2 This function uses the binary search algorithm that has \\(O(log(N))\\) running time. The efficiency of the bisect function: dictionaries The efficiency of the Counter function: Building an in-memory search index using a hash map docs = [ \"the cat is under the table\" , \"the dog is under the table\" , \"cats and dogs smell roses\" , \"Carla eats an apple\" ] index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : #(3) index [ word ] = [ i ] else : index [ word ] . append ( i ) Building an index We iterate over each term in the document We build a list containing the indices where the term appears Now if we want to do search (for example we want to retrieve all documents contain the table term), we can easily use it to query the index: results = index [ \"table\" ] result_documents = [ docs [ i ] for i in results ] And it takes us \\(O(1)\\) time complexity. So we can query any number of documents in constant time. sets Unordered collections of unique elements. Time complexity of union , intersection and difference operations of two sets. Application of sets in practice: boolean search - for example, query all documents contain multiple terms. For example, we may want to search for all the documents that contain the words cat and table . This kind of query can be efficiently computed by taking the intersection between the set of documents containing cat and the set of documents containing table. index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : index [ word ] = { i } #(3) else : index [ word ] . add ( i ) index [ 'cat' ] . intersection ( index [ 'table' ]) #(4) Building an index using sets We iterate over each term in the document We build a set containing the indices where the term appears Querying the documents containing both \"cat\" and \"table\" heaps Heaps are data structures designed to quickly fin and extract the maximum (or minimum) value in a collection. Application of heaps in practice: process of incoming tasks in order of maximum priority. Insertion or extraction of maximum value takes \\(O(log(N))\\) time complexity. import heapq collection = [ 10 , 3 , 3 , 4 , 5 , 6 ] heapq . heapify ( collection ) heapq . heappop ( collection ) #(1) extract the minimum value, returns: 3 Another example of performing insertion and extraction from queue import PriorityQueue queue = PriorityQueue () for element in collection : queue . put ( element ) queue . get () #(1) returns: 3 If the maximum element is required we can just multiply each element of the list by -1, i.e. invert the order of elements. If it is needed to associate an object (for example, task) with each number (and make priority order) we can insert tuples (number, object) : queue = PriorityQueue () queue . put (( 3 , \"priority 3\" )) queue . put (( 2 , \"priority 2\" )) queue . put (( 1 , \"priority 1\" )) queue . get () #(1) returns: (1, \"priority 1\") tries Tries are extremely fast at matching a list of strings against a prefix. This is especially useful when implementing features such as search as you type and autocompletion , where the list of available completions is very large and short response times are required. from patricia import trie from random import choice from string import ascii_uppercase def random_string ( length ): \"\"\"Produce a random string made of *length* uppercase \\ ascii characters\"\"\" return '' . join ( choice ( ascii_uppercase ) for i in \\ range ( length )) strings = [ random_string ( 32 ) for i in range ( 10000 )] strings_dict = { s : 0 for s in strings } #(1) strings_trie = trie ( ** strings_dict ) matches = list ( strings_trie . iter ( 'AA' )) a dictionary where all values are 0 Algorithmic optimization Using caching and memorization to improve efficiency The idea behind caching is to store expensive results in a temporary location ( cache ) that can be located in memory, on disk, or in a remote location. The cache functions are located inside functools python module. See documentation Example of lru_cache() function: from functools import lru_cache @lru_cache () def sum2 ( a , b ): print ( \"Calculating {} + {} \" . format ( a , b )) return a + b print ( sum2 ( 1 , 2 )) #(1) print ( sum2 ( 1 , 2 )) #(2) Output: # Calculating 1 + 2 # 3 Second time function is not called again. Result was cached. Output: # 3 We can restrict max size of cache by @lru_cache(max_size=16) . In this case new values will replace old ones based on strategy least recently used . Other third-party module which allows to implement cache is Joblib Basically it works same as lru_cache() but the results will be stored on disk in the directory specified by the cachedir argument: from joblib import Memory memory = Memory ( cachedir = '/path/to/cachedir' ) @memory . cache def sum2 ( a , b ): return a + b Using iteration with comprehensions and generators To speed up looping we can use comprehensions and generators . Time comparison: def loop (): res = [] for i in range ( 100000 ): res . append ( i * i ) return sum ( res ) def dict_loop (): res = {} for i in range ( 100000 ): res [ i ] = i return res def comprehension (): return sum ([ i * i for i in range ( 100000 )]) def generator (): return sum ( i * i for i in range ( 100000 )) def dict_comprehension (): return { i : i for i in range ( 100000 )} % timeit loop () 100 loops , best of 3 : 16.1 ms per loop % timeit comprehension () 100 loops , best of 3 : 10.1 ms per loop % timeit generator () 100 loops , best of 3 : 12.4 ms per loop % timeit dict_loop () 100 loops , best of 3 : 13.2 ms per loop % timeit dict_comprehension () 100 loops , best of 3 : 12.8 ms per loop Example of efficient looping using filter and map functions in combination with generators: def map_comprehension ( numbers ): a = [ n * 2 for n in numbers ] b = [ n ** 2 for n in a ] c = [ n ** 0.33 for n in b ] return max ( c ) The problem here is that for every list comprehension we are allocating a new list, which increases memory usage. We can solve it by implementing generators: def map_normal ( numbers ): a = map ( lambda n : n * 2 , numbers ) b = map ( lambda n : n ** 2 , a ) c = map ( lambda n : n ** 0.33 , b ) return max ( c ) % load_ext memory_profiler numbers = range ( 1000000 ) % memit map_comprehension ( numbers ) peak memory : 166.33 MiB , increment : 102.54 MiB % memit map_normal ( numbers ) peak memory : 71.04 MiB , increment : 0.00 MiB More about generators Using Numpy for high-speed calculations on big arrays Numpy allows to manipulate multidimensional numerical data and perform mathematical computations that are highly optimized. Using Pandas with database-style data Pandas allows to work with labeled, categorical data (a.k.a dictionaries in Python) Create pd.Series object: import pandas as pd patients = [ 0 , 1 , 2 , 3 ] effective = [ True , True , False , False ] effective_series = pd . Series ( effective , index = patients ) In pandas keys are not limited to integers compared to numpy: patients = [ \"a\" , \"b\" , \"c\" , \"d\" ] effective = [ True , True , False , False ] effective_series = pd . Series ( effective , index = patients ) We can create a dataframe to hold key-value data: patients = [ \"a\" , \"b\" , \"c\" , \"d\" ] columns = { \"systolic_blood_pressure_initial\" : [ 120 , 126 , 130 , 115 ], \"diastolic_blood_pressure_initial\" : [ 75 , 85 , 90 , 87 ], \"systolic_blood_pressure_final\" : [ 115 , 123 , 130 , 118 ], \"diastolic_blood_pressure_final\" : [ 70 , 82 , 92 , 87 ] } df = pd . DataFrame ( columns , index = patients ) Indexing Series and DataFrame objects: Using key of the element effective_series . loc [ \"a\" ] Using position of the element effective_series . iloc [ 0 ] Using position of the element effective_series . iloc [ 0 ] Also pandas supports such useful operations as join for filtering data and extracting specific rows. Using xarray Xarray combines best from Numpy and Pandas - working with labeled multidimensional data. For details see the xarray-documentation Example: import xarray as xr ds = xr . Dataset . from_dataframe ( df ) Using Cython to increase performance Use documentation for detailed overview of its functionality","title":"Efficient Programming"},{"location":"backend/efficient_programming/#efficient-programming-in-python","text":"Sources : Books: Advanced Python Programming","title":"Efficient Programming in Python"},{"location":"backend/efficient_programming/#profiling","text":"Profilers in Python Profiling - technique that allows to pinpoint the most resource-intensive parts of an application. Profiler - program that runs an aplication and monitors how long each function takes to execute, so it detects the functions on which our application spends of its time. pytest-benchmark is used for testing running time of specific functions. See documentation of pytest-benchmark for details. Another profiling tool is cProfile . from simul import benchmark import cProfile pr = cProfile . Profile () pr . enable () benchmark () pr . disable () pr . print_stats () KCachegrind - graphical user interface visualizing profiling output. How to do profiling: Check modules of the app with cProfile or other profiler in order to understand which module is time-consuming Use line_profiler to check functions line by line. See the documentation","title":"Profiling"},{"location":"backend/efficient_programming/#profiling-memory-usage","text":"Memory profiler summarizes the information of memory usage of process.","title":"Profiling memory usage"},{"location":"backend/efficient_programming/#optimizing-code","text":"Methods of optimzation: improve algorithms used minimize the number of instructions","title":"Optimizing code"},{"location":"backend/efficient_programming/#using-the-right-data-structures","text":"Estimation time of operations per data sctructure: list of 10,000 size deque (double-ended queue) of 10,000 size Tip A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the bisect module . bisect allows fast search on sorted arrays. insert bisect collection = [ 1 , 2 , 4 , 5 , 6 ] bisect . bisect ( collection , 3 ) # Result: 2 This function uses the binary search algorithm that has \\(O(log(N))\\) running time. The efficiency of the bisect function: dictionaries The efficiency of the Counter function: Building an in-memory search index using a hash map docs = [ \"the cat is under the table\" , \"the dog is under the table\" , \"cats and dogs smell roses\" , \"Carla eats an apple\" ] index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : #(3) index [ word ] = [ i ] else : index [ word ] . append ( i ) Building an index We iterate over each term in the document We build a list containing the indices where the term appears Now if we want to do search (for example we want to retrieve all documents contain the table term), we can easily use it to query the index: results = index [ \"table\" ] result_documents = [ docs [ i ] for i in results ] And it takes us \\(O(1)\\) time complexity. So we can query any number of documents in constant time. sets Unordered collections of unique elements. Time complexity of union , intersection and difference operations of two sets. Application of sets in practice: boolean search - for example, query all documents contain multiple terms. For example, we may want to search for all the documents that contain the words cat and table . This kind of query can be efficiently computed by taking the intersection between the set of documents containing cat and the set of documents containing table. index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : index [ word ] = { i } #(3) else : index [ word ] . add ( i ) index [ 'cat' ] . intersection ( index [ 'table' ]) #(4) Building an index using sets We iterate over each term in the document We build a set containing the indices where the term appears Querying the documents containing both \"cat\" and \"table\" heaps Heaps are data structures designed to quickly fin and extract the maximum (or minimum) value in a collection. Application of heaps in practice: process of incoming tasks in order of maximum priority. Insertion or extraction of maximum value takes \\(O(log(N))\\) time complexity. import heapq collection = [ 10 , 3 , 3 , 4 , 5 , 6 ] heapq . heapify ( collection ) heapq . heappop ( collection ) #(1) extract the minimum value, returns: 3 Another example of performing insertion and extraction from queue import PriorityQueue queue = PriorityQueue () for element in collection : queue . put ( element ) queue . get () #(1) returns: 3 If the maximum element is required we can just multiply each element of the list by -1, i.e. invert the order of elements. If it is needed to associate an object (for example, task) with each number (and make priority order) we can insert tuples (number, object) : queue = PriorityQueue () queue . put (( 3 , \"priority 3\" )) queue . put (( 2 , \"priority 2\" )) queue . put (( 1 , \"priority 1\" )) queue . get () #(1) returns: (1, \"priority 1\") tries Tries are extremely fast at matching a list of strings against a prefix. This is especially useful when implementing features such as search as you type and autocompletion , where the list of available completions is very large and short response times are required. from patricia import trie from random import choice from string import ascii_uppercase def random_string ( length ): \"\"\"Produce a random string made of *length* uppercase \\ ascii characters\"\"\" return '' . join ( choice ( ascii_uppercase ) for i in \\ range ( length )) strings = [ random_string ( 32 ) for i in range ( 10000 )] strings_dict = { s : 0 for s in strings } #(1) strings_trie = trie ( ** strings_dict ) matches = list ( strings_trie . iter ( 'AA' )) a dictionary where all values are 0","title":"Using the right data structures"},{"location":"backend/efficient_programming/#algorithmic-optimization","text":"","title":"Algorithmic optimization"},{"location":"backend/efficient_programming/#using-caching-and-memorization-to-improve-efficiency","text":"The idea behind caching is to store expensive results in a temporary location ( cache ) that can be located in memory, on disk, or in a remote location. The cache functions are located inside functools python module. See documentation Example of lru_cache() function: from functools import lru_cache @lru_cache () def sum2 ( a , b ): print ( \"Calculating {} + {} \" . format ( a , b )) return a + b print ( sum2 ( 1 , 2 )) #(1) print ( sum2 ( 1 , 2 )) #(2) Output: # Calculating 1 + 2 # 3 Second time function is not called again. Result was cached. Output: # 3 We can restrict max size of cache by @lru_cache(max_size=16) . In this case new values will replace old ones based on strategy least recently used . Other third-party module which allows to implement cache is Joblib Basically it works same as lru_cache() but the results will be stored on disk in the directory specified by the cachedir argument: from joblib import Memory memory = Memory ( cachedir = '/path/to/cachedir' ) @memory . cache def sum2 ( a , b ): return a + b","title":"Using caching and memorization to improve efficiency"},{"location":"backend/efficient_programming/#using-iteration-with-comprehensions-and-generators","text":"To speed up looping we can use comprehensions and generators . Time comparison: def loop (): res = [] for i in range ( 100000 ): res . append ( i * i ) return sum ( res ) def dict_loop (): res = {} for i in range ( 100000 ): res [ i ] = i return res def comprehension (): return sum ([ i * i for i in range ( 100000 )]) def generator (): return sum ( i * i for i in range ( 100000 )) def dict_comprehension (): return { i : i for i in range ( 100000 )} % timeit loop () 100 loops , best of 3 : 16.1 ms per loop % timeit comprehension () 100 loops , best of 3 : 10.1 ms per loop % timeit generator () 100 loops , best of 3 : 12.4 ms per loop % timeit dict_loop () 100 loops , best of 3 : 13.2 ms per loop % timeit dict_comprehension () 100 loops , best of 3 : 12.8 ms per loop Example of efficient looping using filter and map functions in combination with generators: def map_comprehension ( numbers ): a = [ n * 2 for n in numbers ] b = [ n ** 2 for n in a ] c = [ n ** 0.33 for n in b ] return max ( c ) The problem here is that for every list comprehension we are allocating a new list, which increases memory usage. We can solve it by implementing generators: def map_normal ( numbers ): a = map ( lambda n : n * 2 , numbers ) b = map ( lambda n : n ** 2 , a ) c = map ( lambda n : n ** 0.33 , b ) return max ( c ) % load_ext memory_profiler numbers = range ( 1000000 ) % memit map_comprehension ( numbers ) peak memory : 166.33 MiB , increment : 102.54 MiB % memit map_normal ( numbers ) peak memory : 71.04 MiB , increment : 0.00 MiB More about generators","title":"Using iteration with comprehensions and generators"},{"location":"backend/efficient_programming/#using-numpy-for-high-speed-calculations-on-big-arrays","text":"Numpy allows to manipulate multidimensional numerical data and perform mathematical computations that are highly optimized.","title":"Using Numpy for high-speed calculations on big arrays"},{"location":"backend/efficient_programming/#using-pandas-with-database-style-data","text":"Pandas allows to work with labeled, categorical data (a.k.a dictionaries in Python) Create pd.Series object: import pandas as pd patients = [ 0 , 1 , 2 , 3 ] effective = [ True , True , False , False ] effective_series = pd . Series ( effective , index = patients ) In pandas keys are not limited to integers compared to numpy: patients = [ \"a\" , \"b\" , \"c\" , \"d\" ] effective = [ True , True , False , False ] effective_series = pd . Series ( effective , index = patients ) We can create a dataframe to hold key-value data: patients = [ \"a\" , \"b\" , \"c\" , \"d\" ] columns = { \"systolic_blood_pressure_initial\" : [ 120 , 126 , 130 , 115 ], \"diastolic_blood_pressure_initial\" : [ 75 , 85 , 90 , 87 ], \"systolic_blood_pressure_final\" : [ 115 , 123 , 130 , 118 ], \"diastolic_blood_pressure_final\" : [ 70 , 82 , 92 , 87 ] } df = pd . DataFrame ( columns , index = patients ) Indexing Series and DataFrame objects: Using key of the element effective_series . loc [ \"a\" ] Using position of the element effective_series . iloc [ 0 ] Using position of the element effective_series . iloc [ 0 ] Also pandas supports such useful operations as join for filtering data and extracting specific rows.","title":"Using Pandas with database-style data"},{"location":"backend/efficient_programming/#using-xarray","text":"Xarray combines best from Numpy and Pandas - working with labeled multidimensional data. For details see the xarray-documentation Example: import xarray as xr ds = xr . Dataset . from_dataframe ( df )","title":"Using xarray"},{"location":"backend/efficient_programming/#using-cython-to-increase-performance","text":"Use documentation for detailed overview of its functionality","title":"Using Cython to increase performance"},{"location":"backend/message_broker/","text":"Source: Books: David Dossot* Lovisa Johansson - RabbitMQ essentials * build distributed and scalable applications with message queuing using RabbitMQ (2020) RabbitMQ documentation Key Definitions Broker or message broker is a piece of software that receives messages from one application or service, and delivers them to another application, service, or broker. Virtual host, vhost exists within the broker. It's a way to separate applications that are using the same RabbitMQ instance, similar to a logical container inside a broker; for example, separating working environments into development on one vhost and staging on another, keeping them within the same broker instead of setting up multiple brokers. Users, exchanges, queues, and so on are isolated on one specific vhost. A user connected to a particular vhost cannot access any resources (queue, exchange, and so on) from another vhost. Users can have different access privileges to different vhosts. Connection is a physical network (TCP) connection between the application (publisher/consumer) and a broker. When the client disconnects or a system failure occurs, the connection is closed. Channel is a virtual connection inside a connection. It reuses a connection, forgoing the need to reauthorize and open a new TCP stream. When messages are published or consumed, it is done over a channel. Many channels can be established within a single connection. Exchange entity is in charge of applying routing rules for messages, making sure that messages are reaching their final destination. In other words, the exchange ensures that the received message ends up in the correct queues. Which queue the message ends up in depends on the rules defined by the exchange type. A queue needs to be bound to at least one exchange to be able to receive messages. Routing rules include direct (point-to-point), topic (publish-subscribe), fanout (multicast), and header exchanges. Queue is a sequence of items; in this case, messages. The queue exists within the broker. Binding is a virtual link between an exchange and a queue within the broker. It enables messages to flow from an exchange to a queue. Messaging or message queuing is a method of communication between applications or components. Messages are typically small requests, replies, status updates, or even just information. A message queue provides a temporary place for these messages to stay, allowing applications to send and receive them as necessary. RabbitMQ is an open source message broker that acts as the intermediary or middleman for independent applications, giving them a common platform to communicate. RabbitMQ mainly uses an Erlang-based implementation of the Advanced Message Queuing Protocol (AMQP) , which supports advanced features such as clustering and the complex routing of messages. Message queuing is a one-way communication style that provides asynchronous interaction between systems. Most common message exchange pattern is request-response . It is synchronous, so it follows tight-coupling between client and server and has deep impact on the architecture of the whole system (hard to evolve, scale and ship in independent releases). Message-queuing exchange pattern is a one-way style of interaction where one system asynchronously interacts with another system via messages through a message broker. Requesting system doesn't wait for response, it continues processing no matter what. Systems and applications play both the role of message publishers (producers) and message consumers (subscribers). Advantage of using this style of interaction is that systems become loosely coupled with each other. They do not need to know the location of other nodes on the network; a mere name is enough to reach them. The architecture represented via message queuing allows for the following: The publishers or consumers can be updated one by one, without them impacting each other. The performance of each side leaves the other side unaffected. The publishers or consumers are allowed to fail without impacting each other. The number of instances of publishers and consumers to scale and to accommodate their workload in complete independence. Technology mixing between consumer and publishers. Overview of concepts in AMQP: RabbitMQ features RabbitMQ can be used as standalone instance or as a cluster on multiple servers RabbitMQ brokers can be connected together using different techniques, such as federation and shovels, in order to form messaging topologies with smart message routing across brokers and the capacity to span multiple data centers. Message queues between microservices Message queues are often used in between microservices. Microservices are not strictly connected to each other. They instead use message queues to keep in touch. One service asynchronously pushes messages to a queue and those messages are delivered to the correct destination when the consumer is ready. For example, a webstore: Event and tasks Events are notifications that tell applications when something has happened. One application can subscribe to events from another application and respond by creating and handling tasks for themselves. A typical use case is when RabbitMQ acts as a task queue that handles slow operations. Messages are first entering the queue and then handled. New tasks are then added to another queue: Installation on Ubuntu Download the latest releases of the installed software, and verify that curl , apt-transport-https and GnuPG are on the system $ apt upgrade $ sudo apt install curl gnupg -y $ sudo apt install apt-transport-https Ubuntu does not include RabbitMQ by default, so it must be added to the repository key before you proceed. Execute the following set of commands in a Terminal: $ curl -fsSL https://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-rel ease-signing-key.asc sudo apt-key add - sudo tee /etc/apt/sources.list.d/bintray.rabbitmq.list <<EOF deb https://dl.bintray.com/rabbitmq-erlang/debian [os release name] erlang deb https://dl.bintray.com/rabbitmq/debian [os release name] main EOF Install: $ sudo apt install -y rabbitmq-server $ sudo apt install librabbitmq-dev RabbitMQ installation on Docker $ docker pull rabbitmq $ docker run -d --hostname my-rabbit --name my-rabbit -p 5672 :5672 -p 15672:15672 -e RABBITMQ_ERLANG_COOKIE='cookie_for_clustering' -e RABBITMQ_DEFAULT_USER=user -e RABBITMQ_DEFAULT_PASS=password --name some- rabbit rabbitmq:3-management Starting RabbitMQ $ rabbitmq-server start Run the brker as a service: $ sudo systemctl enable rabbitmq-server $ sudo systemctl start rabbitmq-server $ sudo systemctl status rabbitmq-server Veryfying that RabbitMQ is running $ sudo service rabbitmq-server status Installing the management plugin (Web UI) $ sudo rabbitmq-plugins enable rabbitmq_management Enabling plugins on node rabbit@host: rabbitmq_management The following plugins have been configured: rabbitmq_consistent_hash_exchange rabbitmq_event_exchange rabbitmq_federation rabbitmq_management rabbitmq_management_agent rabbitmq_shovel rabbitmq_web_dispatch Applying plugin configuration to rabbit@host... The following plugins have been enabled: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatch Navigate to http://<hostname>:15672 Configure users: $ sudo rabbitmqctl add_user cc-admin taxi123 Adding user \"cc-admin\" ... $ sudo rabbitmqctl set_user_tags cc-admin administrator Setting tags for user \"cc-admin\" to [administrator] ... Change guest default user and default password: $ sudo rabbitmqctl change_password guest guest123 Main dashboard of the management console Create the cc-dev user: $ sudo rabbitmqctl add_user cc-dev taxi123 Adding user \"cc-dev\" ... Create a vhost called cc-dev-vhost : $ sudo rabbitmqctl add_vhost cc-dev-vhost Adding vhost \"cc-dev-vhost\" ... As it stands, neither the cc-admin nor cc-dev users have permission to do anything on cc-dev-vhost . Fix this by giving the vhost full rights, as follows: $ sudo rabbitmqctl set_permissions -p cc-dev-vhost cc-admin \".*\" \".*\" \".*\" Setting permissions for user \"cc-admin\" in vhost \"cc-dev-vhost\" ... $ sudo rabbitmqctl set_permissions -p cc-dev-vhost cc-dev \".*\" \".*\" \".*\" Setting permissions for user \"cc-dev\" in vhost \"cc-dev-vhost\" ... It is a triplet of permissions for the considered vhost, which grants configure , write , and read permissions on the designated resources for the considered user and vhost. See access control commands in the documentation Alternative way is to do it via the management console: Example of RabbitMQ usage in real application Let's say we want to create Taxi App like this: The following steps on the above diagram: A customer uses CC's mobile application to book a taxi. A request is now sent from the mobile application to the Application Service. This request includes information about the trip that the customer wants to book. The Application Service stores the request in a database. The Application Service adds a message with information about the trip to a queue in RabbitMQ. Connected taxi cars subscribe to the message (the booking request). A taxi responds to the customer by sending a message back to RabbitMQ. The Application Service subscribes to the messages. Again, the Application Service stores the information in a database. The Application Service forwards the information to the customer. The taxi app starts to automatically send the taxi's geographical location at a given interval to RabbitMQ. The location of the taxi is then passed straight to the customer's mobile application, via WebSockets, so that they know when the taxi arrives. Direct Exchange routing pattern Topic routing pattern Note A routing pattern consists of several words separated by dots. A best practice to follow is to structure routing keys from the most general element to the most specific one, such as news.economy.usa or europe.sweden.stockholm. The topic exchange supports strict routing key matching and will also perform wildcard matching using * and # as placeholders for exactly one word and zero or more words, respectively. Important points about consumers and queues in RabbitMQ: A queue can have multiple consumers (unless the exclusive tag is used). Each channel can have multiple consumers. Each consumer uses server resources, so it is best to make sure not to use too many consumers. Channels are full-duplex, meaning that one channel can be used to both publish and consume messages. Note There is no logical limit to the number of channels or consumers a RabbitMQ broker can handle. There are, however, limiting factors, such as available memory, broker CPU power, and network bandwidth. As each channel mobilizes memory and consumes CPU power, limiting the number of channels or consumers may be a consideration in some environments. The administrator can configure a maximum number of channels per connection by using the channel_max parameter.","title":"Message Broker"},{"location":"backend/message_broker/#key-definitions","text":"Broker or message broker is a piece of software that receives messages from one application or service, and delivers them to another application, service, or broker. Virtual host, vhost exists within the broker. It's a way to separate applications that are using the same RabbitMQ instance, similar to a logical container inside a broker; for example, separating working environments into development on one vhost and staging on another, keeping them within the same broker instead of setting up multiple brokers. Users, exchanges, queues, and so on are isolated on one specific vhost. A user connected to a particular vhost cannot access any resources (queue, exchange, and so on) from another vhost. Users can have different access privileges to different vhosts. Connection is a physical network (TCP) connection between the application (publisher/consumer) and a broker. When the client disconnects or a system failure occurs, the connection is closed. Channel is a virtual connection inside a connection. It reuses a connection, forgoing the need to reauthorize and open a new TCP stream. When messages are published or consumed, it is done over a channel. Many channels can be established within a single connection. Exchange entity is in charge of applying routing rules for messages, making sure that messages are reaching their final destination. In other words, the exchange ensures that the received message ends up in the correct queues. Which queue the message ends up in depends on the rules defined by the exchange type. A queue needs to be bound to at least one exchange to be able to receive messages. Routing rules include direct (point-to-point), topic (publish-subscribe), fanout (multicast), and header exchanges. Queue is a sequence of items; in this case, messages. The queue exists within the broker. Binding is a virtual link between an exchange and a queue within the broker. It enables messages to flow from an exchange to a queue. Messaging or message queuing is a method of communication between applications or components. Messages are typically small requests, replies, status updates, or even just information. A message queue provides a temporary place for these messages to stay, allowing applications to send and receive them as necessary. RabbitMQ is an open source message broker that acts as the intermediary or middleman for independent applications, giving them a common platform to communicate. RabbitMQ mainly uses an Erlang-based implementation of the Advanced Message Queuing Protocol (AMQP) , which supports advanced features such as clustering and the complex routing of messages. Message queuing is a one-way communication style that provides asynchronous interaction between systems. Most common message exchange pattern is request-response . It is synchronous, so it follows tight-coupling between client and server and has deep impact on the architecture of the whole system (hard to evolve, scale and ship in independent releases). Message-queuing exchange pattern is a one-way style of interaction where one system asynchronously interacts with another system via messages through a message broker. Requesting system doesn't wait for response, it continues processing no matter what. Systems and applications play both the role of message publishers (producers) and message consumers (subscribers). Advantage of using this style of interaction is that systems become loosely coupled with each other. They do not need to know the location of other nodes on the network; a mere name is enough to reach them. The architecture represented via message queuing allows for the following: The publishers or consumers can be updated one by one, without them impacting each other. The performance of each side leaves the other side unaffected. The publishers or consumers are allowed to fail without impacting each other. The number of instances of publishers and consumers to scale and to accommodate their workload in complete independence. Technology mixing between consumer and publishers. Overview of concepts in AMQP:","title":"Key Definitions"},{"location":"backend/message_broker/#rabbitmq-features","text":"RabbitMQ can be used as standalone instance or as a cluster on multiple servers RabbitMQ brokers can be connected together using different techniques, such as federation and shovels, in order to form messaging topologies with smart message routing across brokers and the capacity to span multiple data centers.","title":"RabbitMQ features"},{"location":"backend/message_broker/#message-queues-between-microservices","text":"Message queues are often used in between microservices. Microservices are not strictly connected to each other. They instead use message queues to keep in touch. One service asynchronously pushes messages to a queue and those messages are delivered to the correct destination when the consumer is ready. For example, a webstore:","title":"Message queues between microservices"},{"location":"backend/message_broker/#event-and-tasks","text":"Events are notifications that tell applications when something has happened. One application can subscribe to events from another application and respond by creating and handling tasks for themselves. A typical use case is when RabbitMQ acts as a task queue that handles slow operations. Messages are first entering the queue and then handled. New tasks are then added to another queue:","title":"Event and tasks"},{"location":"backend/message_broker/#installation-on-ubuntu","text":"Download the latest releases of the installed software, and verify that curl , apt-transport-https and GnuPG are on the system $ apt upgrade $ sudo apt install curl gnupg -y $ sudo apt install apt-transport-https Ubuntu does not include RabbitMQ by default, so it must be added to the repository key before you proceed. Execute the following set of commands in a Terminal: $ curl -fsSL https://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-rel ease-signing-key.asc sudo apt-key add - sudo tee /etc/apt/sources.list.d/bintray.rabbitmq.list <<EOF deb https://dl.bintray.com/rabbitmq-erlang/debian [os release name] erlang deb https://dl.bintray.com/rabbitmq/debian [os release name] main EOF Install: $ sudo apt install -y rabbitmq-server $ sudo apt install librabbitmq-dev","title":"Installation on Ubuntu"},{"location":"backend/message_broker/#rabbitmq-installation-on-docker","text":"$ docker pull rabbitmq $ docker run -d --hostname my-rabbit --name my-rabbit -p 5672 :5672 -p 15672:15672 -e RABBITMQ_ERLANG_COOKIE='cookie_for_clustering' -e RABBITMQ_DEFAULT_USER=user -e RABBITMQ_DEFAULT_PASS=password --name some- rabbit rabbitmq:3-management","title":"RabbitMQ installation on Docker"},{"location":"backend/message_broker/#starting-rabbitmq","text":"$ rabbitmq-server start Run the brker as a service: $ sudo systemctl enable rabbitmq-server $ sudo systemctl start rabbitmq-server $ sudo systemctl status rabbitmq-server","title":"Starting RabbitMQ"},{"location":"backend/message_broker/#veryfying-that-rabbitmq-is-running","text":"$ sudo service rabbitmq-server status","title":"Veryfying that RabbitMQ is running"},{"location":"backend/message_broker/#installing-the-management-plugin-web-ui","text":"$ sudo rabbitmq-plugins enable rabbitmq_management Enabling plugins on node rabbit@host: rabbitmq_management The following plugins have been configured: rabbitmq_consistent_hash_exchange rabbitmq_event_exchange rabbitmq_federation rabbitmq_management rabbitmq_management_agent rabbitmq_shovel rabbitmq_web_dispatch Applying plugin configuration to rabbit@host... The following plugins have been enabled: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatch Navigate to http://<hostname>:15672 Configure users: $ sudo rabbitmqctl add_user cc-admin taxi123 Adding user \"cc-admin\" ... $ sudo rabbitmqctl set_user_tags cc-admin administrator Setting tags for user \"cc-admin\" to [administrator] ... Change guest default user and default password: $ sudo rabbitmqctl change_password guest guest123 Main dashboard of the management console Create the cc-dev user: $ sudo rabbitmqctl add_user cc-dev taxi123 Adding user \"cc-dev\" ... Create a vhost called cc-dev-vhost : $ sudo rabbitmqctl add_vhost cc-dev-vhost Adding vhost \"cc-dev-vhost\" ... As it stands, neither the cc-admin nor cc-dev users have permission to do anything on cc-dev-vhost . Fix this by giving the vhost full rights, as follows: $ sudo rabbitmqctl set_permissions -p cc-dev-vhost cc-admin \".*\" \".*\" \".*\" Setting permissions for user \"cc-admin\" in vhost \"cc-dev-vhost\" ... $ sudo rabbitmqctl set_permissions -p cc-dev-vhost cc-dev \".*\" \".*\" \".*\" Setting permissions for user \"cc-dev\" in vhost \"cc-dev-vhost\" ... It is a triplet of permissions for the considered vhost, which grants configure , write , and read permissions on the designated resources for the considered user and vhost. See access control commands in the documentation Alternative way is to do it via the management console:","title":"Installing the management plugin (Web UI)"},{"location":"backend/message_broker/#example-of-rabbitmq-usage-in-real-application","text":"Let's say we want to create Taxi App like this: The following steps on the above diagram: A customer uses CC's mobile application to book a taxi. A request is now sent from the mobile application to the Application Service. This request includes information about the trip that the customer wants to book. The Application Service stores the request in a database. The Application Service adds a message with information about the trip to a queue in RabbitMQ. Connected taxi cars subscribe to the message (the booking request). A taxi responds to the customer by sending a message back to RabbitMQ. The Application Service subscribes to the messages. Again, the Application Service stores the information in a database. The Application Service forwards the information to the customer. The taxi app starts to automatically send the taxi's geographical location at a given interval to RabbitMQ. The location of the taxi is then passed straight to the customer's mobile application, via WebSockets, so that they know when the taxi arrives. Direct Exchange routing pattern Topic routing pattern Note A routing pattern consists of several words separated by dots. A best practice to follow is to structure routing keys from the most general element to the most specific one, such as news.economy.usa or europe.sweden.stockholm. The topic exchange supports strict routing key matching and will also perform wildcard matching using * and # as placeholders for exactly one word and zero or more words, respectively. Important points about consumers and queues in RabbitMQ: A queue can have multiple consumers (unless the exclusive tag is used). Each channel can have multiple consumers. Each consumer uses server resources, so it is best to make sure not to use too many consumers. Channels are full-duplex, meaning that one channel can be used to both publish and consume messages. Note There is no logical limit to the number of channels or consumers a RabbitMQ broker can handle. There are, however, limiting factors, such as available memory, broker CPU power, and network bandwidth. As each channel mobilizes memory and consumes CPU power, limiting the number of channels or consumers may be a consideration in some environments. The administrator can configure a maximum number of channels per connection by using the channel_max parameter.","title":"Example of RabbitMQ usage in real application"},{"location":"backend/oop/","text":"Object-Oriented Programming with Python Sources: Books: Steven F. Lott, Dusty Phillips - Python Object-Oriented Programming_ Build robust and maintainable object-oriented Python applications and libraries-Packt Publishing (2021) Introduction to object-oriented Object - collection of data and associated behaviours Object-oriented programming means writing code directed toward modeling objects. It is defined by describing a collection of interacting objects via their data and behavior. Object-oriented design (OOD) is the process of converting requirements into an implementation specification. The designer must name the objects, define the behaviors, and formally specify which objects can activate specific behaviors on other objects. The design stage is all about transforming what should be done into how it should be done. Object-oriented programming (OOP) is the process of converting a design into a working program that does what the product owner originally requested. Class describes related objects. Data represents the individual characteristics of a certain object, its current state. property means particular kind of attribute. Methods - behaviours that can beperformed on a specific class of object. The difference from usual functions is that methods have access to the attributes of the class. Method is a function attached to a specific class. The self parameter refers to a specific instance of the class. The key purpose of modeling an object is to determine what the public interface of that object will be. Interface - collection of attributes and methods that other objects can access to interact with that object. Encapsulation - process of wrapping (sometimes hiding) the implementation of an object. Encapsulation is, literally, creating a capsule (or wrapper) on the attributes. Abstraction - means dealing with the level of detail that is most appropriate to a given task. It is the process of extracting a public interface from the inner details. Example of abstraction: Composition - the act of collecting several objects together to create a new one. Aggregation - similar to composition but the difference is that aggregated objects can exist independently. Where to implement Composition and where Aggregation: If the composite (outside) object controls when the related (inside) objects are created and destroyed, composition is most suitable. If the related object is created independently of the composite object, or can outlast that object, an aggregate relationship makes more sense. Inheritance - is like a family tree. One class can inherit attributes and methods from another class. Abstract method say this: \"We demand this method exist in any non-abstract subclass, but we are declining to specify an implementation in this class.\" It says what the class should do, but provides no advice on how to do it. Polymorphism is the ability to treat a class differently, depending on which subclass is implemented. Multiple inheritance allows a subclass to inherit functionality from multiple parent classes. Python has a defined method resolution order (MRO) to help us understand which of the alternative methods will be used. Example of class implementation class Point : def __init__ ( self , x : float = 0 , y : float = 0 ) -> None : self . move ( x , y ) def move ( self , x : float , y : float ) -> None : self . x = x self . y = y def reset ( self ) -> None : self . move ( 0 , 0 ) def calculate_distance ( self , other : \"Point\" ) -> float : return math . hypot ( self . x - other . x , self . y - other . y ) point = Point ( 3 , 5 ) # Constructing a Point instance Example of abstract class: from typing import Optional class Formatter : def format ( self , string : str ) -> str : pass def format_string ( string : str , formatter : Optional [ Formatter ] = None ) -> str : \"\"\" Format a string using the formatter object, which is expected to have a format() method that accepts a string. \"\"\" class DefaultFormatter ( Formatter ): \"\"\"Format a string in title case.\"\"\" def format ( self , string : str ) -> str : return str ( string ) . title () if not formatter : formatter = DefaultFormatter () return formatter . format ( string ) The __repr__() method is used to create a representation of the object. This representation is a string that generally has the syntax of a Python expression to rebuild the object. For simple numbers, it's the number. For a simple string, it will include the quotes. For more complex objects, it will have all the necessary Python punctuation, including all the details of the class and state of the object. class Sample : def __init__ ( self , sepal_length : float , sepal_width : float , petal_length : float , petal_width : float , species : Optional [ str ] = None , ) -> None : self . sepal_length = sepal_length self . sepal_width = sepal_width self . petal_length = petal_length self . petal_width = petal_width self . species = species self . classification : Optional [ str ] = None def __repr__ ( self ) -> str : if self . species is None : known_unknown = \"UnknownSample\" else : known_unknown = \"KnownSample\" if self . classification is None : classification = \"\" else : classification = f \", { self . classification } \" return ( f \" { known_unknown } (\" f \"sepal_length= { self . sepal_length } , \" f \"sepal_width= { self . sepal_width } , \" f \"petal_length= { self . petal_length } , \" f \"petal_width= { self . petal_width } , \" f \"species= { self . species !r} \" f \" { classification } \" f \")\" ) We use inherritance for adding new behaviour to existing classes. For changing behaviour by overriding a method of superclass with a new method (with the same name). However, to execute the original __init__() method of superclass we can call it like super().__init__() class Friend ( Contact ): def __init__ ( self , name : str , email : str , phone : str ) -> None : super () . __init__ ( name , email ) #(1) self . phone = phone #(2) first it binds the instance to the parent class using super() and calls __init__() on that object, passing in the expected arguments. it then does its own initialization, namely, setting the phone attribute, which is unique to the Friend class. Multiple inheritance class EmailableContact ( Contact , MailSender ): pass Polymorphism implementation example from pathlib import Path class AudioFile : ext : str def __init__ ( self , filepath : Path ) -> None : if not filepath . suffix == self . ext : raise ValueError ( \"Invalid file format\" ) self . filepath = filepath class MP3File ( AudioFile ): ext = \".mp3\" def play ( self ) -> None : print ( f \"playing { self . filepath } as mp3\" ) class WavFile ( AudioFile ): ext = \".wav\" def play ( self ) -> None : print ( f \"playing { self . filepath } as wav\" ) class OggFile ( AudioFile ): ext = \".ogg\" def play ( self ) -> None : print ( f \"playing { self . filepath } as ogg\" ) SOLID design principles S . Single Responsibility Principle. A class should have one responsibility. This can mean one reason to change when the application's requirements change. O . Open/Closed. A class should be open to extension but closed to modification. L . Liskov Substitution. (Named after Barbara Liskov, who created one of the first object-oriented programming languages, CLU.) Any subclass can be substituted for its superclass. This tends to focus a class hierarchy on classes that have very similar interfaces, leading to polymorphism among the objects. This the essence of inheritance. I . Interface Segregation. A class should have the smallest interface possible. Classes should be relatively small and isolated. D . Dependency Inversion. Pragmatically, we'd like classes to be independent, so a Liskov Substitution doesn't involve a lot of code changes. In Python, this often means referring to superclasses in type hints to be sure we have the flexibility to make changes. In some cases, it also means providing parameters so that we can make global class changes without revising any of the code. Handling exceptions def funnier_division ( divisor : int ) -> Union [ str , float ]: try : if divisor == 13 : raise ValueError ( \"13 is an unlucky number\" ) return 100 / divisor except ( ZeroDivisionError , TypeError ): return \"Enter a number other than zero\" More complex example with finally : some_exceptions = [ ValueError , TypeError , IndexError , None ] for choice in some_exceptions : try : print ( f \" \\n Raising { choice } \" ) if choice : raise choice ( \"An error\" ) else : print ( \"no exception raised\" ) except ValueError : print ( \"Caught a ValueError\" ) except TypeError : print ( \"Caught a TypeError\" ) except Exception as e : print ( f \"Caught some other error: { e . __class__ . __name__ } \" ) else : print ( \"This code called if there is no exception\" ) finally : print ( \"This cleanup code is always called\" ) Examples of performing tasks in the finally section: Cleaning up an open database connection Closing an open file Sending a closing handshake over the network Note The finally clause is executed after the return statement inside a try clause. Also, pay attention to the output when no exception is raised: both the else and the finally clauses are executed. The exception hierarchy The Exception class actually extends a class called BaseException . In fact, all exceptions must extend the BaseException class or one of its subclasses. The hierarchy is: Custom exception from decimal import Decimal class InvalidWithdrawal ( ValueError ): def __init__ ( self , balance : Decimal , amount : Decimal ) -> None : super () . __init__ ( f \"account doesn't have $ { amount } \" ) self . amount = amount self . balance = balance def overage ( self ) -> Decimal : return self . amount - self . balance Main recommendations to build classes Use methods to represent actions; things that can be done to, or performed by, the object. When you call a method, even with only one argument, it should do something. Method names are generally verbs. Use attributes or properties to represent the state of the object. These are the nouns, adjectives, and prepositions that describe an object. Default to ordinary (non-property) attributes, initialized in the __init__() method. These must be computed eagerly,which is a good starting point for any design. Use properties for attributes in the exceptional case when there's a computation involved with setting or getting (or deleting) an attribute. Examples include data validation, logging, and access controls. We'll look at cache management in a moment. We can also use properties for lazy attributes, where we want to defer the computation because it's costly and rarely needed. Note Good names and docstrings are essential. Each class, method, function, variable, property, attribute, module, and package name should be chosen thoughtfully. When writing docstrings, don't explain how the code works (the code should do that). Be sure to focus on what the code's purpose is, what the preconditions are for using it, and what will be true after the function or method has been used.","title":"OOP"},{"location":"backend/oop/#object-oriented-programming-with-python","text":"Sources: Books: Steven F. Lott, Dusty Phillips - Python Object-Oriented Programming_ Build robust and maintainable object-oriented Python applications and libraries-Packt Publishing (2021)","title":"Object-Oriented Programming with Python"},{"location":"backend/oop/#introduction-to-object-oriented","text":"Object - collection of data and associated behaviours Object-oriented programming means writing code directed toward modeling objects. It is defined by describing a collection of interacting objects via their data and behavior. Object-oriented design (OOD) is the process of converting requirements into an implementation specification. The designer must name the objects, define the behaviors, and formally specify which objects can activate specific behaviors on other objects. The design stage is all about transforming what should be done into how it should be done. Object-oriented programming (OOP) is the process of converting a design into a working program that does what the product owner originally requested. Class describes related objects. Data represents the individual characteristics of a certain object, its current state. property means particular kind of attribute. Methods - behaviours that can beperformed on a specific class of object. The difference from usual functions is that methods have access to the attributes of the class. Method is a function attached to a specific class. The self parameter refers to a specific instance of the class. The key purpose of modeling an object is to determine what the public interface of that object will be. Interface - collection of attributes and methods that other objects can access to interact with that object. Encapsulation - process of wrapping (sometimes hiding) the implementation of an object. Encapsulation is, literally, creating a capsule (or wrapper) on the attributes. Abstraction - means dealing with the level of detail that is most appropriate to a given task. It is the process of extracting a public interface from the inner details. Example of abstraction: Composition - the act of collecting several objects together to create a new one. Aggregation - similar to composition but the difference is that aggregated objects can exist independently. Where to implement Composition and where Aggregation: If the composite (outside) object controls when the related (inside) objects are created and destroyed, composition is most suitable. If the related object is created independently of the composite object, or can outlast that object, an aggregate relationship makes more sense. Inheritance - is like a family tree. One class can inherit attributes and methods from another class. Abstract method say this: \"We demand this method exist in any non-abstract subclass, but we are declining to specify an implementation in this class.\" It says what the class should do, but provides no advice on how to do it. Polymorphism is the ability to treat a class differently, depending on which subclass is implemented. Multiple inheritance allows a subclass to inherit functionality from multiple parent classes. Python has a defined method resolution order (MRO) to help us understand which of the alternative methods will be used.","title":"Introduction to object-oriented"},{"location":"backend/oop/#example-of-class-implementation","text":"class Point : def __init__ ( self , x : float = 0 , y : float = 0 ) -> None : self . move ( x , y ) def move ( self , x : float , y : float ) -> None : self . x = x self . y = y def reset ( self ) -> None : self . move ( 0 , 0 ) def calculate_distance ( self , other : \"Point\" ) -> float : return math . hypot ( self . x - other . x , self . y - other . y ) point = Point ( 3 , 5 ) # Constructing a Point instance Example of abstract class: from typing import Optional class Formatter : def format ( self , string : str ) -> str : pass def format_string ( string : str , formatter : Optional [ Formatter ] = None ) -> str : \"\"\" Format a string using the formatter object, which is expected to have a format() method that accepts a string. \"\"\" class DefaultFormatter ( Formatter ): \"\"\"Format a string in title case.\"\"\" def format ( self , string : str ) -> str : return str ( string ) . title () if not formatter : formatter = DefaultFormatter () return formatter . format ( string ) The __repr__() method is used to create a representation of the object. This representation is a string that generally has the syntax of a Python expression to rebuild the object. For simple numbers, it's the number. For a simple string, it will include the quotes. For more complex objects, it will have all the necessary Python punctuation, including all the details of the class and state of the object. class Sample : def __init__ ( self , sepal_length : float , sepal_width : float , petal_length : float , petal_width : float , species : Optional [ str ] = None , ) -> None : self . sepal_length = sepal_length self . sepal_width = sepal_width self . petal_length = petal_length self . petal_width = petal_width self . species = species self . classification : Optional [ str ] = None def __repr__ ( self ) -> str : if self . species is None : known_unknown = \"UnknownSample\" else : known_unknown = \"KnownSample\" if self . classification is None : classification = \"\" else : classification = f \", { self . classification } \" return ( f \" { known_unknown } (\" f \"sepal_length= { self . sepal_length } , \" f \"sepal_width= { self . sepal_width } , \" f \"petal_length= { self . petal_length } , \" f \"petal_width= { self . petal_width } , \" f \"species= { self . species !r} \" f \" { classification } \" f \")\" ) We use inherritance for adding new behaviour to existing classes. For changing behaviour by overriding a method of superclass with a new method (with the same name). However, to execute the original __init__() method of superclass we can call it like super().__init__() class Friend ( Contact ): def __init__ ( self , name : str , email : str , phone : str ) -> None : super () . __init__ ( name , email ) #(1) self . phone = phone #(2) first it binds the instance to the parent class using super() and calls __init__() on that object, passing in the expected arguments. it then does its own initialization, namely, setting the phone attribute, which is unique to the Friend class. Multiple inheritance class EmailableContact ( Contact , MailSender ): pass","title":"Example of class implementation"},{"location":"backend/oop/#polymorphism-implementation-example","text":"from pathlib import Path class AudioFile : ext : str def __init__ ( self , filepath : Path ) -> None : if not filepath . suffix == self . ext : raise ValueError ( \"Invalid file format\" ) self . filepath = filepath class MP3File ( AudioFile ): ext = \".mp3\" def play ( self ) -> None : print ( f \"playing { self . filepath } as mp3\" ) class WavFile ( AudioFile ): ext = \".wav\" def play ( self ) -> None : print ( f \"playing { self . filepath } as wav\" ) class OggFile ( AudioFile ): ext = \".ogg\" def play ( self ) -> None : print ( f \"playing { self . filepath } as ogg\" )","title":"Polymorphism implementation example"},{"location":"backend/oop/#solid-design-principles","text":"S . Single Responsibility Principle. A class should have one responsibility. This can mean one reason to change when the application's requirements change. O . Open/Closed. A class should be open to extension but closed to modification. L . Liskov Substitution. (Named after Barbara Liskov, who created one of the first object-oriented programming languages, CLU.) Any subclass can be substituted for its superclass. This tends to focus a class hierarchy on classes that have very similar interfaces, leading to polymorphism among the objects. This the essence of inheritance. I . Interface Segregation. A class should have the smallest interface possible. Classes should be relatively small and isolated. D . Dependency Inversion. Pragmatically, we'd like classes to be independent, so a Liskov Substitution doesn't involve a lot of code changes. In Python, this often means referring to superclasses in type hints to be sure we have the flexibility to make changes. In some cases, it also means providing parameters so that we can make global class changes without revising any of the code.","title":"SOLID design principles"},{"location":"backend/oop/#handling-exceptions","text":"def funnier_division ( divisor : int ) -> Union [ str , float ]: try : if divisor == 13 : raise ValueError ( \"13 is an unlucky number\" ) return 100 / divisor except ( ZeroDivisionError , TypeError ): return \"Enter a number other than zero\" More complex example with finally : some_exceptions = [ ValueError , TypeError , IndexError , None ] for choice in some_exceptions : try : print ( f \" \\n Raising { choice } \" ) if choice : raise choice ( \"An error\" ) else : print ( \"no exception raised\" ) except ValueError : print ( \"Caught a ValueError\" ) except TypeError : print ( \"Caught a TypeError\" ) except Exception as e : print ( f \"Caught some other error: { e . __class__ . __name__ } \" ) else : print ( \"This code called if there is no exception\" ) finally : print ( \"This cleanup code is always called\" ) Examples of performing tasks in the finally section: Cleaning up an open database connection Closing an open file Sending a closing handshake over the network Note The finally clause is executed after the return statement inside a try clause. Also, pay attention to the output when no exception is raised: both the else and the finally clauses are executed.","title":"Handling exceptions"},{"location":"backend/oop/#the-exception-hierarchy","text":"The Exception class actually extends a class called BaseException . In fact, all exceptions must extend the BaseException class or one of its subclasses. The hierarchy is:","title":"The exception hierarchy"},{"location":"backend/oop/#custom-exception","text":"from decimal import Decimal class InvalidWithdrawal ( ValueError ): def __init__ ( self , balance : Decimal , amount : Decimal ) -> None : super () . __init__ ( f \"account doesn't have $ { amount } \" ) self . amount = amount self . balance = balance def overage ( self ) -> Decimal : return self . amount - self . balance","title":"Custom exception"},{"location":"backend/oop/#main-recommendations-to-build-classes","text":"Use methods to represent actions; things that can be done to, or performed by, the object. When you call a method, even with only one argument, it should do something. Method names are generally verbs. Use attributes or properties to represent the state of the object. These are the nouns, adjectives, and prepositions that describe an object. Default to ordinary (non-property) attributes, initialized in the __init__() method. These must be computed eagerly,which is a good starting point for any design. Use properties for attributes in the exceptional case when there's a computation involved with setting or getting (or deleting) an attribute. Examples include data validation, logging, and access controls. We'll look at cache management in a moment. We can also use properties for lazy attributes, where we want to defer the computation because it's costly and rarely needed. Note Good names and docstrings are essential. Each class, method, function, variable, property, attribute, module, and package name should be chosen thoughtfully. When writing docstrings, don't explain how the code works (the code should do that). Be sure to focus on what the code's purpose is, what the preconditions are for using it, and what will be true after the function or method has been used.","title":"Main recommendations to build classes"},{"location":"backend/testing/","text":"Python Testing Sources: Books: Python Testing with pytest, Second Edition Documentation: pytest Main Concepts of Testing Test Strategy Determing Test Scope We will almost always want to test the behavior of the user visible functionality. However, there are quite a few other questions we need to consider when determining how much testing we need to do: Is security a concern? Performance? Do interactions need to be fast? How fast? Loading? Can you handle lots of people with lots of requests? Are you expecting to need to? If so, you should test for that. Input validation? For really any system that accepts input from users, we should validate the data before acting on it. Decide what features to test? Considering Software Architecture how your project\u2019s software is organized what APIs are available what the interfaces are where code complexity lives modularity etc. Another main factors that affect testing strategy: At what level should we be testing? The top user interface? Something lower? Subsystem? All levels? How easy is it to test at different levels? Who is responsible for the different levels and the testing of each? If you are supplying a subsystem, are you only responsible for that subsystem? Is someone else doing the system testing? If so, it\u2019s an easy choice: test your own subsystem. Evaluating the features to test Prioritize features to test based on the following factors: Recent \u2014 New features, new areas of code, new functionality that has been recently repaired, refactored, or otherwise modified Core \u2014 Your product\u2019s unique selling propositions (USPs). The essential functions that must continue to work in order for the product to be useful Risk \u2014 Areas of the application that pose more risk, such as areas important to customers but not used regularly by the development team or parts that use third-party code you don\u2019t quite trust Problematic \u2014 Functionality that frequently breaks or often gets defect reports against it Expertise \u2014 Features or algorithms understood by a limited subset of people Creating test cases Start with a non-trivial, \u201chappy path\u201d test case. Then look at test cases that represent interesting sets of input, interesting starting states, interesting end states, or all possible error states. Coverage Code coverage tool cannot tell you if your test suite is good; it can only tell you how much of the application code is getting hit by your test suite. But that in itself is useful information. $ pip install coverage $ pip install pytest-cov Commands pytest -v test_two.py #(1) pytest --tb = no #(2) pytest --tb = no test_one.py test_two.py #(3) -v adds information if test fails and shows exactly the reason of failure use --tb=no flag to rutn off tracebacks indicate specific tests to run Test naming conventions Test files should be named test_<something>.py or <something>_test.py . Test methods and functions should be named test_<something> . Test classes should be named Test<Something> Test exceptions If we need to test expected exceptions we can use pytest.raises() : import pytest import cards def test_no_path_raises (): with pytest . raises ( TypeError ): #(1) cards . CardsDB () def test_raises_with_info (): #(2) match_regex = \"missing 1 .* positional argument\" with pytest . raises ( TypeError , match = match_regex ): cards . CardsDB () def test_raises_with_info_alt (): with pytest . raises ( TypeError ) as exc_info : cards . CardsDB () expected = \"missing 1 required positional argument\" assert expected in str ( exc_info . value ) the with pytest.raises(TypeError): statement says that whatever is in the next block of code should raise a TypeError exception. If no exception is raised, the test fails. If the test raises a different exception, it fails. check to make sure the message is correct, or any other aspect of the exception, like additional parameters Structuring test functions def test_to_dict (): # GIVEN a Card object with known contents c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) #(1) # WHEN we call to_dict() on the object c2 = c1 . to_dict () #(2) # THEN the result will be a dictionary with known content c2_expected = { \"summary\" : \"something\" , \"owner\" : \"brian\" , \"state\" : \"todo\" , \"id\" : 123 , } assert c2 == c2_expected #(3) A starting state. This is where you set up data or the environment to get ready for the action. Some action is performed. This is the focus of the test\u2014the behavior we are trying to make sure is working right. Some expected result or end state should happen. At the end of the test, we make sure the action resulted in the expected behavior. Structure tests in classes: class TestEquality : def test_equality ( self ): c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) c2 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) assert c1 == c2 def test_equality_with_diff_ids ( self ): c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) c2 = Card ( \"something\" , \"brian\" , \"todo\" , 4567 ) assert c1 == c2 def test_inequality ( self ): c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) c2 = Card ( \"completely different\" , \"okken\" , \"done\" , 123 ) assert c1 != c2 Running a single test method, test class, or module: $ pytest ch2/test_classes.py::TestEquality::test_equality $ pytest ch2/test_classes.py::TestEquality $ pytest ch2/test_classes.py Running a single test function or module: $ pytest ch2/test_card.py::test_defaults $ pytest ch2/test_card.py Running the whole directory: $ pytest ch2 Pytest Fixtures Pytest treats exceptions differently during fixtures compared to during a test function. An exception (or assert failure or call to pytest.fail() ) that happens during the test code proper results in a \u201cFail\u201d result. However, during a fixture, the test function is reported as \u201cError.\u201d This distinction is helpful when debugging why a test didn\u2019t pass. If a test results in \u201cFail,\u201d the failure is somewhere in the test function (or something the function called). If a test results in \u201cError,\u201d the failure is somewhere in a fixture. import pytest @pytest . fixture () def cards_db (): #(1) with TemporaryDirectory () as db_dir : db_path = Path ( db_dir ) db = cards . CardsDB ( db_path ) yield db #(2) db . close () def test_empty ( cards_db ): assert cards_db . count () == 0 def test_two ( cards_db ): cards_db . add_card ( cards . Card ( \"first\" )) cards_db . add_card ( cards . Card ( \"second\" )) assert cards_db . count () == 2 cards_db fixture is \u201csetting up\u201d for the test by getting the database ready. It is like a dependency, which is injected into the test function. Fixture functions run before the tests that use them. If there is a yield in the function, it stops there, passes control to the tests, and picks up on the next line after the tests are done. The following command-line flag --setup-show shows us the order of operations of tests and fixtures: $ pytest --setup-show test_count.py ======================== test session starts ========================= collected 2 items test_count.py SETUP F cards_db ch3/test_count.py::test_empty (fixtures used: cards_db). TEARDOWN F cards_db SETUP F cards_db ch3/test_count.py::test_two (fixtures used: cards_db). TEARDOWN F cards_db ========================= 2 passed in 0.02s ========================== Fixture scope: @pytest . fixture ( scope = \"module\" ) #(1) def cards_db (): with TemporaryDirectory () as db_dir : db_path = Path ( db_dir ) db = cards . CardsDB ( db_path ) yield db db . close () scope=\"module\" allows us to open database only once and use the result in the whole module Using scope allows us to run slow part of the test once for multiple tests: $ pytest --setup-show test_mod_scope.py ========================== test session starts ========================== collected 2 items test_mod_scope.py SETUP M cards_db ch3/test_mod_scope.py::test_empty (fixtures used: cards_db). ch3/test_mod_scope.py::test_two (fixtures used: cards_db). TEARDOWN M cards_db =========================== 2 passed in 0.03s =========================== scope options: scope='function' - Run once per test function. The setup portion is run before each test using the fixture. scope='class' - Run once per test class, regardless of how many test methods are in the class. scope='module' - Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='package' - Run once per package, or test directory, regardless of how many test functions or methods or other fixtures in the package use it. scope='session' - Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call. The conftest.py file is considered by pytest as a \u201clocal plugin\u201d and can contain hook functions and fixtures. Thus we can share fixtures among other tests. Tests can use any fixture that is in the same test module as a test function, or in a conftest.py file in the same directory, or in any level of parent directory up to the root of the tests. $ pytest --fixtures -v ... -------------------- fixtures defined from conftest --------------------- cards_db [session scope] -- conftest.py:7 CardsDB object connected to a temporary database ... Builtin Fixtures The tmp_path and tmp_path_factory fixtures are used to create temporary directories. The tmp_path function-scope fixture returns a pathlib.Path instance that points to a temporary directory that sticks around during your test and a bit longer. The tmp_path_factory session-scope fixture returns a TempPathFactory object. This object has a mktemp() function that returns Path objects. We can use mktemp() to create multiple temporary directories. Example: def test_tmp_path ( tmp_path ): file = tmp_path / \"file.txt\" file . write_text ( \"Hello\" ) assert file . read_text () == \"Hello\" def test_tmp_path_factory ( tmp_path_factory ): path = tmp_path_factory . mktemp ( \"sub\" ) file = path / \"file.txt\" file . write_text ( \"Hello\" ) assert file . read_text () == \"Hello\" Parametrization The @pytest.mark.parametrize() decorator is used to define the sets of arguments to pass to the test: import pytest from cards import Card @pytest . mark . parametrize ( \"start_summary, start_state\" , #(1) [ ( \"write a book\" , \"done\" ), #(2) ( \"second edition\" , \"in prog\" ), ( \"create a course\" , \"todo\" ), ], ) def test_finish ( cards_db , start_summary , start_state ): initial_card = Card ( summary = start_summary , state = start_state ) index = cards_db . add_card ( initial_card ) cards_db . finish ( index ) card = cards_db . get_card ( index ) assert card . state == \"done\" list of names of the parameters. It can be list of strings, comma-separated strings. our list of test cases. Each element in the list is a test case represented by a tuple or list that has one element for each argument that gets sent to the test function. Result: $ pytest -v test_func_param.py::test_finish ========================= test session starts ========================== collected 3 items test_func_param.py::test_finish[write a book-done] PASSED [ 33%] test_func_param.py::test_finish[second edition-in prog] PASSED [ 66%] test_func_param.py::test_finish[create a course-todo] PASSED [100%] ========================== 3 passed in 0.05s =========================== Fixture paramentization: @pytest . fixture ( params = [ \"done\" , \"in prog\" , \"todo\" ]) #(1) def start_state ( request ): return request . param def test_finish ( cards_db , start_state ): c = Card ( \"write a book\" , state = start_state ) index = cards_db . add_card ( c ) cards_db . finish ( index ) card = cards_db . get_card ( index ) assert card . state == \"done\" pytest will then call the fixture once each for every set of values we provide. Then downstream, every test function that depends on the fixture will be called, once each for every fixture value. Mocking The mock package is used to swap out pieces of the system to isolate bits of our application code from the rest of the system.","title":"Testing"},{"location":"backend/testing/#python-testing","text":"Sources: Books: Python Testing with pytest, Second Edition Documentation: pytest","title":"Python Testing"},{"location":"backend/testing/#main-concepts-of-testing","text":"","title":"Main Concepts of Testing"},{"location":"backend/testing/#test-strategy","text":"","title":"Test Strategy"},{"location":"backend/testing/#determing-test-scope","text":"We will almost always want to test the behavior of the user visible functionality. However, there are quite a few other questions we need to consider when determining how much testing we need to do: Is security a concern? Performance? Do interactions need to be fast? How fast? Loading? Can you handle lots of people with lots of requests? Are you expecting to need to? If so, you should test for that. Input validation? For really any system that accepts input from users, we should validate the data before acting on it. Decide what features to test?","title":"Determing Test Scope"},{"location":"backend/testing/#considering-software-architecture","text":"how your project\u2019s software is organized what APIs are available what the interfaces are where code complexity lives modularity etc. Another main factors that affect testing strategy: At what level should we be testing? The top user interface? Something lower? Subsystem? All levels? How easy is it to test at different levels? Who is responsible for the different levels and the testing of each? If you are supplying a subsystem, are you only responsible for that subsystem? Is someone else doing the system testing? If so, it\u2019s an easy choice: test your own subsystem.","title":"Considering Software Architecture"},{"location":"backend/testing/#evaluating-the-features-to-test","text":"Prioritize features to test based on the following factors: Recent \u2014 New features, new areas of code, new functionality that has been recently repaired, refactored, or otherwise modified Core \u2014 Your product\u2019s unique selling propositions (USPs). The essential functions that must continue to work in order for the product to be useful Risk \u2014 Areas of the application that pose more risk, such as areas important to customers but not used regularly by the development team or parts that use third-party code you don\u2019t quite trust Problematic \u2014 Functionality that frequently breaks or often gets defect reports against it Expertise \u2014 Features or algorithms understood by a limited subset of people","title":"Evaluating the features to test"},{"location":"backend/testing/#creating-test-cases","text":"Start with a non-trivial, \u201chappy path\u201d test case. Then look at test cases that represent interesting sets of input, interesting starting states, interesting end states, or all possible error states.","title":"Creating test cases"},{"location":"backend/testing/#coverage","text":"Code coverage tool cannot tell you if your test suite is good; it can only tell you how much of the application code is getting hit by your test suite. But that in itself is useful information. $ pip install coverage $ pip install pytest-cov","title":"Coverage"},{"location":"backend/testing/#commands","text":"pytest -v test_two.py #(1) pytest --tb = no #(2) pytest --tb = no test_one.py test_two.py #(3) -v adds information if test fails and shows exactly the reason of failure use --tb=no flag to rutn off tracebacks indicate specific tests to run","title":"Commands"},{"location":"backend/testing/#test-naming-conventions","text":"Test files should be named test_<something>.py or <something>_test.py . Test methods and functions should be named test_<something> . Test classes should be named Test<Something>","title":"Test naming conventions"},{"location":"backend/testing/#test-exceptions","text":"If we need to test expected exceptions we can use pytest.raises() : import pytest import cards def test_no_path_raises (): with pytest . raises ( TypeError ): #(1) cards . CardsDB () def test_raises_with_info (): #(2) match_regex = \"missing 1 .* positional argument\" with pytest . raises ( TypeError , match = match_regex ): cards . CardsDB () def test_raises_with_info_alt (): with pytest . raises ( TypeError ) as exc_info : cards . CardsDB () expected = \"missing 1 required positional argument\" assert expected in str ( exc_info . value ) the with pytest.raises(TypeError): statement says that whatever is in the next block of code should raise a TypeError exception. If no exception is raised, the test fails. If the test raises a different exception, it fails. check to make sure the message is correct, or any other aspect of the exception, like additional parameters","title":"Test exceptions"},{"location":"backend/testing/#structuring-test-functions","text":"def test_to_dict (): # GIVEN a Card object with known contents c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) #(1) # WHEN we call to_dict() on the object c2 = c1 . to_dict () #(2) # THEN the result will be a dictionary with known content c2_expected = { \"summary\" : \"something\" , \"owner\" : \"brian\" , \"state\" : \"todo\" , \"id\" : 123 , } assert c2 == c2_expected #(3) A starting state. This is where you set up data or the environment to get ready for the action. Some action is performed. This is the focus of the test\u2014the behavior we are trying to make sure is working right. Some expected result or end state should happen. At the end of the test, we make sure the action resulted in the expected behavior. Structure tests in classes: class TestEquality : def test_equality ( self ): c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) c2 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) assert c1 == c2 def test_equality_with_diff_ids ( self ): c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) c2 = Card ( \"something\" , \"brian\" , \"todo\" , 4567 ) assert c1 == c2 def test_inequality ( self ): c1 = Card ( \"something\" , \"brian\" , \"todo\" , 123 ) c2 = Card ( \"completely different\" , \"okken\" , \"done\" , 123 ) assert c1 != c2 Running a single test method, test class, or module: $ pytest ch2/test_classes.py::TestEquality::test_equality $ pytest ch2/test_classes.py::TestEquality $ pytest ch2/test_classes.py Running a single test function or module: $ pytest ch2/test_card.py::test_defaults $ pytest ch2/test_card.py Running the whole directory: $ pytest ch2","title":"Structuring test functions"},{"location":"backend/testing/#pytest-fixtures","text":"Pytest treats exceptions differently during fixtures compared to during a test function. An exception (or assert failure or call to pytest.fail() ) that happens during the test code proper results in a \u201cFail\u201d result. However, during a fixture, the test function is reported as \u201cError.\u201d This distinction is helpful when debugging why a test didn\u2019t pass. If a test results in \u201cFail,\u201d the failure is somewhere in the test function (or something the function called). If a test results in \u201cError,\u201d the failure is somewhere in a fixture. import pytest @pytest . fixture () def cards_db (): #(1) with TemporaryDirectory () as db_dir : db_path = Path ( db_dir ) db = cards . CardsDB ( db_path ) yield db #(2) db . close () def test_empty ( cards_db ): assert cards_db . count () == 0 def test_two ( cards_db ): cards_db . add_card ( cards . Card ( \"first\" )) cards_db . add_card ( cards . Card ( \"second\" )) assert cards_db . count () == 2 cards_db fixture is \u201csetting up\u201d for the test by getting the database ready. It is like a dependency, which is injected into the test function. Fixture functions run before the tests that use them. If there is a yield in the function, it stops there, passes control to the tests, and picks up on the next line after the tests are done. The following command-line flag --setup-show shows us the order of operations of tests and fixtures: $ pytest --setup-show test_count.py ======================== test session starts ========================= collected 2 items test_count.py SETUP F cards_db ch3/test_count.py::test_empty (fixtures used: cards_db). TEARDOWN F cards_db SETUP F cards_db ch3/test_count.py::test_two (fixtures used: cards_db). TEARDOWN F cards_db ========================= 2 passed in 0.02s ========================== Fixture scope: @pytest . fixture ( scope = \"module\" ) #(1) def cards_db (): with TemporaryDirectory () as db_dir : db_path = Path ( db_dir ) db = cards . CardsDB ( db_path ) yield db db . close () scope=\"module\" allows us to open database only once and use the result in the whole module Using scope allows us to run slow part of the test once for multiple tests: $ pytest --setup-show test_mod_scope.py ========================== test session starts ========================== collected 2 items test_mod_scope.py SETUP M cards_db ch3/test_mod_scope.py::test_empty (fixtures used: cards_db). ch3/test_mod_scope.py::test_two (fixtures used: cards_db). TEARDOWN M cards_db =========================== 2 passed in 0.03s =========================== scope options: scope='function' - Run once per test function. The setup portion is run before each test using the fixture. scope='class' - Run once per test class, regardless of how many test methods are in the class. scope='module' - Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='package' - Run once per package, or test directory, regardless of how many test functions or methods or other fixtures in the package use it. scope='session' - Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call. The conftest.py file is considered by pytest as a \u201clocal plugin\u201d and can contain hook functions and fixtures. Thus we can share fixtures among other tests. Tests can use any fixture that is in the same test module as a test function, or in a conftest.py file in the same directory, or in any level of parent directory up to the root of the tests. $ pytest --fixtures -v ... -------------------- fixtures defined from conftest --------------------- cards_db [session scope] -- conftest.py:7 CardsDB object connected to a temporary database ...","title":"Pytest Fixtures"},{"location":"backend/testing/#builtin-fixtures","text":"The tmp_path and tmp_path_factory fixtures are used to create temporary directories. The tmp_path function-scope fixture returns a pathlib.Path instance that points to a temporary directory that sticks around during your test and a bit longer. The tmp_path_factory session-scope fixture returns a TempPathFactory object. This object has a mktemp() function that returns Path objects. We can use mktemp() to create multiple temporary directories. Example: def test_tmp_path ( tmp_path ): file = tmp_path / \"file.txt\" file . write_text ( \"Hello\" ) assert file . read_text () == \"Hello\" def test_tmp_path_factory ( tmp_path_factory ): path = tmp_path_factory . mktemp ( \"sub\" ) file = path / \"file.txt\" file . write_text ( \"Hello\" ) assert file . read_text () == \"Hello\"","title":"Builtin Fixtures"},{"location":"backend/testing/#parametrization","text":"The @pytest.mark.parametrize() decorator is used to define the sets of arguments to pass to the test: import pytest from cards import Card @pytest . mark . parametrize ( \"start_summary, start_state\" , #(1) [ ( \"write a book\" , \"done\" ), #(2) ( \"second edition\" , \"in prog\" ), ( \"create a course\" , \"todo\" ), ], ) def test_finish ( cards_db , start_summary , start_state ): initial_card = Card ( summary = start_summary , state = start_state ) index = cards_db . add_card ( initial_card ) cards_db . finish ( index ) card = cards_db . get_card ( index ) assert card . state == \"done\" list of names of the parameters. It can be list of strings, comma-separated strings. our list of test cases. Each element in the list is a test case represented by a tuple or list that has one element for each argument that gets sent to the test function. Result: $ pytest -v test_func_param.py::test_finish ========================= test session starts ========================== collected 3 items test_func_param.py::test_finish[write a book-done] PASSED [ 33%] test_func_param.py::test_finish[second edition-in prog] PASSED [ 66%] test_func_param.py::test_finish[create a course-todo] PASSED [100%] ========================== 3 passed in 0.05s =========================== Fixture paramentization: @pytest . fixture ( params = [ \"done\" , \"in prog\" , \"todo\" ]) #(1) def start_state ( request ): return request . param def test_finish ( cards_db , start_state ): c = Card ( \"write a book\" , state = start_state ) index = cards_db . add_card ( c ) cards_db . finish ( index ) card = cards_db . get_card ( index ) assert card . state == \"done\" pytest will then call the fixture once each for every set of values we provide. Then downstream, every test function that depends on the fixture will be called, once each for every fixture value.","title":"Parametrization"},{"location":"backend/testing/#mocking","text":"The mock package is used to swap out pieces of the system to isolate bits of our application code from the rest of the system.","title":"Mocking"},{"location":"frontend/","text":"","title":"Frontend"},{"location":"machine_learning/","text":"Basic Concepts of Machine Learning","title":"Machine Learning"},{"location":"machine_learning/#basic-concepts-of-machine-learning","text":"","title":"Basic Concepts of Machine Learning"},{"location":"machine_learning/math/","text":"Mnemonic storytelling for learning math concepts Source: - Book: Mathematics for Machine Learning - ChatGPT Story one: Once upon a time, in a land far, far away, there were four pillars that held up the castle of machine learning. These four pillars were called Regression, Dimensionality Reduction, Density Estimation, and Classification. Regression was the first pillar and it was the strongest of them all. It was responsible for making predictions about the future based on past data. It was said that if you wanted to know what the weather would be like tomorrow, you would ask Regression. Dimensionality Reduction was the second pillar, and it was just as important as the first. It was responsible for taking large and complex data sets and simplifying them, making them easier to understand and work with. Without Dimensionality Reduction, the castle of machine learning would be overrun by data that no one could make sense of. Density Estimation was the third pillar, and it was the most mysterious of them all. It was responsible for understanding how data was distributed and finding patterns in it. Many people in the castle would spend hours trying to understand the secrets of Density Estimation, but only a select few truly mastered it. Finally, there was Classification, the fourth and final pillar. It was responsible for sorting data into different categories, making it easy to understand and work with. Without Classification, the castle of machine learning would be a chaotic mess of data that no one could make sense of. Together, these four pillars held up the castle of machine learning, and it was said that as long as they stood strong, the kingdom of artificial intelligence would thrive.","title":"Math"},{"location":"machine_learning/math/#mnemonic-storytelling-for-learning-math-concepts","text":"Source: - Book: Mathematics for Machine Learning - ChatGPT Story one: Once upon a time, in a land far, far away, there were four pillars that held up the castle of machine learning. These four pillars were called Regression, Dimensionality Reduction, Density Estimation, and Classification. Regression was the first pillar and it was the strongest of them all. It was responsible for making predictions about the future based on past data. It was said that if you wanted to know what the weather would be like tomorrow, you would ask Regression. Dimensionality Reduction was the second pillar, and it was just as important as the first. It was responsible for taking large and complex data sets and simplifying them, making them easier to understand and work with. Without Dimensionality Reduction, the castle of machine learning would be overrun by data that no one could make sense of. Density Estimation was the third pillar, and it was the most mysterious of them all. It was responsible for understanding how data was distributed and finding patterns in it. Many people in the castle would spend hours trying to understand the secrets of Density Estimation, but only a select few truly mastered it. Finally, there was Classification, the fourth and final pillar. It was responsible for sorting data into different categories, making it easy to understand and work with. Without Classification, the castle of machine learning would be a chaotic mess of data that no one could make sense of. Together, these four pillars held up the castle of machine learning, and it was said that as long as they stood strong, the kingdom of artificial intelligence would thrive.","title":"Mnemonic storytelling for learning math concepts"},{"location":"machine_learning/nlp/","text":"Natural Language Processing Sources Overview Books: Ekaterina Kochmar - Getting Started with Natural Language Processing-Manning (2022) Sample Use Cases Information search Information retrieval Basics In NLP important concept is a vector. Characters can be represented as vectors, words, or documents can be represented as vectors. For example, two documents can be represented as vectors based on the number of occurences of specific words. Example Schema Python Implementation doc1 = \"meeting ... management ... meeting ... management ... meeting\" # (1) doc1 += \"... management ... meeting ... meeting\" vector = [ 0 , 0 ] # (2) for word in doc1 . split ( \" \" ): if word == \"management\" : vector [ 0 ] = vector [ 0 ] + 1 # (3) if word == \"meeting\" : vector [ 1 ] = vector [ 1 ] + 1 # (4) print ( vector ) # (5) represents a document based on keywords only initializes array vector count for \"management\" is incremented in cell 0 count for \"meeting\" is incremented in cell 1. [3, 5] Two vectors can be compared between each other by either length or by measuring angle between them. Info Cosine similarity estimates the similarity between two nonzero vectors in space (or two texts represented by such vectors) on the basis of the angle between these vectors \u2014 for example, the cosine of 0\u00b0 equals 1, which denotes the maximum similarity, and the cosine of 180\u00b0 equals \u20131, which is the lowest value. Unlike Euclidean distance, this measure is not affected by vector length.","title":"NLP"},{"location":"machine_learning/nlp/#natural-language-processing","text":"","title":"Natural Language Processing"},{"location":"machine_learning/nlp/#sources-overview","text":"Books: Ekaterina Kochmar - Getting Started with Natural Language Processing-Manning (2022)","title":"Sources Overview"},{"location":"machine_learning/nlp/#sample-use-cases","text":"Information search Information retrieval","title":"Sample Use Cases"},{"location":"machine_learning/nlp/#basics","text":"In NLP important concept is a vector. Characters can be represented as vectors, words, or documents can be represented as vectors. For example, two documents can be represented as vectors based on the number of occurences of specific words. Example Schema Python Implementation doc1 = \"meeting ... management ... meeting ... management ... meeting\" # (1) doc1 += \"... management ... meeting ... meeting\" vector = [ 0 , 0 ] # (2) for word in doc1 . split ( \" \" ): if word == \"management\" : vector [ 0 ] = vector [ 0 ] + 1 # (3) if word == \"meeting\" : vector [ 1 ] = vector [ 1 ] + 1 # (4) print ( vector ) # (5) represents a document based on keywords only initializes array vector count for \"management\" is incremented in cell 0 count for \"meeting\" is incremented in cell 1. [3, 5] Two vectors can be compared between each other by either length or by measuring angle between them. Info Cosine similarity estimates the similarity between two nonzero vectors in space (or two texts represented by such vectors) on the basis of the angle between these vectors \u2014 for example, the cosine of 0\u00b0 equals 1, which denotes the maximum similarity, and the cosine of 180\u00b0 equals \u20131, which is the lowest value. Unlike Euclidean distance, this measure is not affected by vector length.","title":"Basics"},{"location":"machine_learning/nlp/information_search/","text":"Information Search / Information Retrieval This section explains search algorithm from beginning to end. Understanding the Purpose of the Project Information search algorithm in a nutshel: Boolean search algorithm It is simple search algo that selects all documents that contain any of the words from the query Preprocessing step import nltk import string from nltk import word_tokenize from nltk.corpus import stopwords from nltk.stem.lancaster import LancasterStemmer def process ( text ): stoplist = set ( stopwords . words ( 'english' )) st = LancasterStemmer () word_list = [ st . stem ( word ) for word in word_tokenize ( text . lower ()) if not word in stoplist and not word in string . punctuation ] return word_list word_list = process ( documents . get ( \"27\" )) print ( word_list ) word_list = process ( \"organize, organizing, organizational, organ, organic, organizer\" ) print ( word_list ) # Example: # Input = \"Cost Analysis and Simulation Procedures for the Evaluation of Large Information Systems ...\" # Output = ['cost', 'analys', 'sim', 'proc', 'evalu', 'larg', 'inform','system', ...] Information weighting Code to estimate term frequency in documents and queries: def get_terms ( text ): stoplist = set ( stopwords . words ( 'english' )) terms = {} st = LancasterStemmer () word_list = [ st . stem ( word ) for word in word_tokenize ( text . lower ()) if not word in stoplist and not word in string . punctuation ] for word in word_list : terms [ word ] = terms . get ( word , 0 ) + 1 return terms doc_terms = {} qry_terms = {} for doc_id in documents . keys (): doc_terms [ doc_id ] = get_terms ( documents . get ( doc_id )) for qry_id in queries . keys (): qry_terms [ qry_id ] = get_terms ( queries . get ( qry_id )) print ( len ( doc_terms )) print ( doc_terms . get ( \"1\" )) print ( len ( doc_terms . get ( \"1\" ))) print ( len ( qry_terms )) print ( qry_terms . get ( \"1\" )) print ( len ( qry_terms . get ( \"1\" ))) Code to represent the data in a shared space: def collect_vocabulary (): # (1) all_terms = [] for doc_id in doc_terms . keys (): for term in doc_terms . get ( doc_id ) . keys (): all_terms . append ( term ) for qry_id in qry_terms . keys (): for term in qry_terms . get ( qry_id ) . keys (): all_terms . append ( term ) return sorted ( set ( all_terms )) all_terms = collect_vocabulary () print ( len ( all_terms )) print ( all_terms [: 10 ]) # (2) def vectorize ( input_features , vocabulary ): output = {} for item_id in input_features . keys (): features = input_features . get ( item_id ) output_vector = [] for word in vocabulary : # (3) if word in features . keys (): output_vector . append ( int ( features . get ( word ))) else : output_vector . append ( 0 ) output [ item_id ] = output_vector return output doc_vectors = vectorize ( doc_terms , all_terms ) qry_vectors = vectorize ( qry_terms , all_terms ) # (4) print ( len ( doc_vectors )) print ( len ( doc_vectors . get ( \"1460\" ))) print ( len ( qry_vectors )) print ( len ( qry_vectors . get ( \"112\" ))) # (5) Collect the shared vocabulary of terms from documents and queries and return it as a sorted list. Print out the length of the shared vocabulary and check the first several terms in the vocabulary. Represents each query and each document with a dictionary with the same set of keys Using the vectorize method, you can represent all queries and documents in this shared space. Print out some statistics on these data structures. Vector representation of documents and query: Weighing words with inverse document frequency idf ( \"inform\" ) = log10 ( 1460 / ( 651 + 1 )) \u2248 0.35 idf ( \"dissemin\" ) = log10 ( 1460 / ( 68 + 1 )) \u2248 1.33 The general formula: \\[idf(term) = log_{10} \\ ( \\frac{N}{df(term) + 1})\\] N - total number of documents in the collection","title":"Information Search"},{"location":"machine_learning/nlp/information_search/#information-search-information-retrieval","text":"This section explains search algorithm from beginning to end.","title":"Information Search / Information Retrieval"},{"location":"machine_learning/nlp/information_search/#understanding-the-purpose-of-the-project","text":"Information search algorithm in a nutshel: Boolean search algorithm It is simple search algo that selects all documents that contain any of the words from the query","title":"Understanding the Purpose of the Project"},{"location":"machine_learning/nlp/information_search/#preprocessing-step","text":"import nltk import string from nltk import word_tokenize from nltk.corpus import stopwords from nltk.stem.lancaster import LancasterStemmer def process ( text ): stoplist = set ( stopwords . words ( 'english' )) st = LancasterStemmer () word_list = [ st . stem ( word ) for word in word_tokenize ( text . lower ()) if not word in stoplist and not word in string . punctuation ] return word_list word_list = process ( documents . get ( \"27\" )) print ( word_list ) word_list = process ( \"organize, organizing, organizational, organ, organic, organizer\" ) print ( word_list ) # Example: # Input = \"Cost Analysis and Simulation Procedures for the Evaluation of Large Information Systems ...\" # Output = ['cost', 'analys', 'sim', 'proc', 'evalu', 'larg', 'inform','system', ...]","title":"Preprocessing step"},{"location":"machine_learning/nlp/information_search/#information-weighting","text":"Code to estimate term frequency in documents and queries: def get_terms ( text ): stoplist = set ( stopwords . words ( 'english' )) terms = {} st = LancasterStemmer () word_list = [ st . stem ( word ) for word in word_tokenize ( text . lower ()) if not word in stoplist and not word in string . punctuation ] for word in word_list : terms [ word ] = terms . get ( word , 0 ) + 1 return terms doc_terms = {} qry_terms = {} for doc_id in documents . keys (): doc_terms [ doc_id ] = get_terms ( documents . get ( doc_id )) for qry_id in queries . keys (): qry_terms [ qry_id ] = get_terms ( queries . get ( qry_id )) print ( len ( doc_terms )) print ( doc_terms . get ( \"1\" )) print ( len ( doc_terms . get ( \"1\" ))) print ( len ( qry_terms )) print ( qry_terms . get ( \"1\" )) print ( len ( qry_terms . get ( \"1\" ))) Code to represent the data in a shared space: def collect_vocabulary (): # (1) all_terms = [] for doc_id in doc_terms . keys (): for term in doc_terms . get ( doc_id ) . keys (): all_terms . append ( term ) for qry_id in qry_terms . keys (): for term in qry_terms . get ( qry_id ) . keys (): all_terms . append ( term ) return sorted ( set ( all_terms )) all_terms = collect_vocabulary () print ( len ( all_terms )) print ( all_terms [: 10 ]) # (2) def vectorize ( input_features , vocabulary ): output = {} for item_id in input_features . keys (): features = input_features . get ( item_id ) output_vector = [] for word in vocabulary : # (3) if word in features . keys (): output_vector . append ( int ( features . get ( word ))) else : output_vector . append ( 0 ) output [ item_id ] = output_vector return output doc_vectors = vectorize ( doc_terms , all_terms ) qry_vectors = vectorize ( qry_terms , all_terms ) # (4) print ( len ( doc_vectors )) print ( len ( doc_vectors . get ( \"1460\" ))) print ( len ( qry_vectors )) print ( len ( qry_vectors . get ( \"112\" ))) # (5) Collect the shared vocabulary of terms from documents and queries and return it as a sorted list. Print out the length of the shared vocabulary and check the first several terms in the vocabulary. Represents each query and each document with a dictionary with the same set of keys Using the vectorize method, you can represent all queries and documents in this shared space. Print out some statistics on these data structures. Vector representation of documents and query:","title":"Information weighting"},{"location":"machine_learning/nlp/information_search/#weighing-words-with-inverse-document-frequency","text":"idf ( \"inform\" ) = log10 ( 1460 / ( 651 + 1 )) \u2248 0.35 idf ( \"dissemin\" ) = log10 ( 1460 / ( 68 + 1 )) \u2248 1.33 The general formula: \\[idf(term) = log_{10} \\ ( \\frac{N}{df(term) + 1})\\] N - total number of documents in the collection","title":"Weighing words with inverse document frequency"},{"location":"machine_learning/nlp/statistical_text_analysis/","text":"Text Analysis Sources: Books: Blueprints for Text Analytics Using Python","title":"Statistical Analysis of Text"},{"location":"machine_learning/nlp/statistical_text_analysis/#text-analysis","text":"Sources: Books: Blueprints for Text Analytics Using Python","title":"Text Analysis"},{"location":"machine_learning/nlp/text_classification/","text":"Text Classification In this section the knowledge about text classification is summarized based on the spam filtering project example. Info Classification refers to the process of identifying which category or class among the set of categories (classes) an observation belongs to based on its properties. In machine learning, such properties are called features and the class names are called class labels . If you classify observations into two classes, you are dealing with binary classification ; tasks with more than two classes are examples of multi-class classification . Understanding the Purpose of the Project Provided by client : dataset of spam and normal emails from the past. Task : build a spam filter which can predict if any future incoming email is spam or not. Questions to think about What is the format of provided data? How to use provided data? What features of the emails might be useful, how to extract them? What are sequence of steps in the application? Suggested pipeline of the project Define classes Split text of each email into words Extract useful features Train a classifier Test and evaluate 1. Define classes Define the label of each class and which data represents which class. 2. Split into words Preprocess the raw text from email in order to prepare it for machine learning classifier. Split text into words by whitespaces and punctuation: text = 'Define which data represents \"ham\" class and which data represents \"spam\" class for the machine learning algorithm.' delimiters = [ '\"' , \".\" ] #(1) words = [] current_word = \"\" #(2) for char in text : if char == \" \" : if not current_word == \"\" : words . append ( current_word ) current_word = \"\" #(3) elif char in delimiters : if current_word == \"\" : #(4) words . append ( char ) else : words . append ( current_word ) words . append ( char ) current_word = \"\" #(5) else : current_word += char #(6) print ( words ) initialize list of delimiters the current_word variable keeps track of the word currently being processed. check if the character is a whitespace and the current_word is not empty check if the character is one of the punctuation marks and there is nothing stored in the current_word yet check if the character is one of the punctuation marks and there is information stored in current_word check if the character is any other letter; that is, not specified as a delimiter and not a whitespace To solve the problem with cases like U.S.A. and U.K. which under the above preprocessor are splitted into ['U', '.', 'S', '.', 'A', '.'] and ['U', '.', 'K', '.'], we can use tokenizer. Info Tokenization is the process of word token identification or extraction from the running text. It is often the first step in text preprocessing. Whitespaces and punctuation marks often serve as reliable word separators; however, simple approaches are likely to run into exceptions like \u201cU.S.A.\u201d and similar. Tokenizers are NLP tools that are highly optimized for the task of word tokenization, and they may rely on carefully crafted regular expressions or may be trained using machine-learning algorithms. 3. Extract and normalize features Put all words to lowercase. 4. Train a classifier Choose machine learning model and train it on the preprocessed data. Split dataset into train and test before training (typically 80% for training, and 20% for testing). Na\u00efve Bayes - probabilistic classifier, it makes class prediction based on the estimation which outcome is most likely. The probability in this case is conditional , because it depends on condition (words as features). If P(spam | content) = 0.58 and P(ham | content) = 0.42, predict spam If P(spam | content) = 0.37 and P(ham | content) = 0.63, predict ham How it works on practice Estimate the probability that an email is spam or ham based on its content, taking the number of times this content lead to a particular outcome: \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{num(spam\\ emails\\ with\\ 'Participate\\ in\\ our\\ lottery!')}{num(all\\ email\\ with\\ 'Participate\\ in\\ our\\ lottery!')} \\] In general the formula looks like this: \\[ \\operatorname{P}(outcome\\ |{condition}) = \\frac{num\\ of\\ times(condition\\ led\\ to\\ outcome)}{num\\ of\\ times(condition\\ applied)} \\] The problem here is that we are estimating of particular combination of words (as features), but it may not be many times when we meet exactly the same combination. Therefore, we may not generalise the classifier efficiently. The solution is to split the estimation into smaller bits and establish the link between individual features. Estimate probability by using two formulas: Schema Math Formula 1 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all emails with this content. \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P('Participate\\ in\\ our\\ lottery!'\\ is\\ used\\ in\\ an\\ email)} \\] Formula 2 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all spam emails. \\[ \\operatorname{P}({'Participate\\ in\\ our\\ lottery!'|spam}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P(an\\ email\\ is\\ spam)} \\] General formula (combination of the two previous ones) : \\[ \\operatorname{P}({class|content}) = \\frac{P(content\\ in\\ class)}{P(content)} = \\frac{P(content|class)\\times P(class)}{P(content)} \\] Info P(class) - probability of each class, i.e. distribution of the class in the dataset. This forms prior probability for the classifier. If the dataset is unbalanced, potential probability is inclined respectively. P(content|class) - likelihood that we see this particular content given that the email is spam or ham. Na\u00efve Bayes assumes that the features are independent of each other, i.e. chances of seeing a word 'lottery' in an email are independent of seeing a word 'new' or any other word. Therefore we can estimate the probability of the whole sequence of features by product of probabilities of each feature in this class: \\[ \\operatorname{P}({['participate': True,\\ 'in': True,\\ ...,\\ '!': True]|spam}) = {P('participate': True|spam)\\times P('in': True|spam)\\times ...\\times P('!': True|spam)} \\] And general formula is: \\[ \\operatorname{P}({[f_1, f_2, ..., f_n]|class}) = {P(f_1|class)\\times P(f_2|class)\\times ...\\times P(f_n|class)} \\] How exactly classifier algorithm works Training Phase Test Phase Learns prior probabilities This is simply initial class distribution. For example: P(ham) = 0.71 P(spam) = 0.29 Learns probabilities for each feature given in the classes. This is the proportion of emails with each feature in each class. For example: P ('meeting': True|ham) = 0.50 Predict new email class based on the probabilities of each feature of the content: \\[ \\begin{align} \\begin{cases} P(f_1|spam)\\times ...\\times P(f_n|spam)\\times P(spam) \\geq P(f_1|ham)\\times ...\\times P(f_n|ham)\\times P(ham) \\implies spam\\\\ otherwise \\implies ham\\ \\end{cases} \\end{align} \\] Code implementation from nltk import NaiveBayesClassifier , classify def train ( features , proportion ): train_size = int ( len ( features ) * proportion ) #(1) train_set = features [: train_size ] test_set = features [ train_size :] print ( f \"Training set size = { str ( len ( train_set )) } emails\" ) print ( f \"Test set size = { str ( len ( test_set )) } emails\" ) classifier = NaiveBayesClassifier . train ( train_set ) #(2) return train_set , test_set , classifier train_set , test_set , classifier = train ( all_features , 0.8 ) #(3) split dataset into train and test parts initialize a classifier apply the train function using 80% of dataset for training 5. Classifier evaluation \\[ \\operatorname{Accuracy} = \\frac{num(correct\\ predictions)}{num(all\\ test\\ instances)} \\] Code implementation Define an estimation function which estimates the accuracy of the classifier on each dataset def evaluate ( train_set , test_set , classifier ): print ( f \"Accuracy on the training set = { str ( classify . accuracy ( classifier , train_set )) } \" ) print ( f \"Accuracy of the test set = { str ( classify . accuracy ( classifier , test_set )) } \" ) classifier . show_most_informative_features ( 50 ) #(1) evaluate ( train_set , test_set , classifier ) print top 50 most informative features (words). The words that are most strongly associated with a particular class. It is calculated as \\(max[P(word: True | ham) / P(word: True | spam)]\\) for most predictive ham features, and \\(max[P(word: True | spam) / P(word: True | ham)]\\) for most predictive spam features (check out NLTK\u2019s documentation for more information). Check the contexts of specific words from nltk.text import Text def concordance ( data_list , search_word ): for email in data_list : word_list = [ word for word in word_tokenize ( email . lower ())] text_list = Text ( word_list ) if search_word in word_list : text_list . concordance ( search_word ) print ( \"STOCKS in HAM:\" ) concordance ( ham_list , 'stocks' ) print ( \" \\n\\n STOCKS in SPAM:\" ) concordance ( spam_list , 'stocks' ) 6. Use spam filter on new emails in practice Here is the code for inference of the model on new emails. test_spam_list = [ \"Participate in our new lottery!\" , \"Try out this new medicine\" ] test_ham_list = [ \"See the minutes from the last meeting attached\" , \"Investors are coming to our office on Monday\" ] test_emails = [( email_content , \"spam\" ) for email_content in test_spam_list ] test_emails += [( email_content , \"ham\" ) for email_content in test_ham_list ] new_test_set = [( get_features ( email ), label ) for ( email , label ) in test_emails ] evaluate ( train_set , new_test_set , classifier ) while True : email = input ( \"Type in your email here (or press 'Enter'): \" ) if len ( email ) == 0 : break else : prediction = classifier . classify ( get_features ( email )) print ( f \"This email is likely { prediction } \\n \" )","title":"Text Classification"},{"location":"machine_learning/nlp/text_classification/#text-classification","text":"In this section the knowledge about text classification is summarized based on the spam filtering project example. Info Classification refers to the process of identifying which category or class among the set of categories (classes) an observation belongs to based on its properties. In machine learning, such properties are called features and the class names are called class labels . If you classify observations into two classes, you are dealing with binary classification ; tasks with more than two classes are examples of multi-class classification .","title":"Text Classification"},{"location":"machine_learning/nlp/text_classification/#understanding-the-purpose-of-the-project","text":"Provided by client : dataset of spam and normal emails from the past. Task : build a spam filter which can predict if any future incoming email is spam or not.","title":"Understanding the Purpose of the Project"},{"location":"machine_learning/nlp/text_classification/#questions-to-think-about","text":"What is the format of provided data? How to use provided data? What features of the emails might be useful, how to extract them? What are sequence of steps in the application?","title":"Questions to think about"},{"location":"machine_learning/nlp/text_classification/#suggested-pipeline-of-the-project","text":"Define classes Split text of each email into words Extract useful features Train a classifier Test and evaluate 1. Define classes Define the label of each class and which data represents which class. 2. Split into words Preprocess the raw text from email in order to prepare it for machine learning classifier. Split text into words by whitespaces and punctuation: text = 'Define which data represents \"ham\" class and which data represents \"spam\" class for the machine learning algorithm.' delimiters = [ '\"' , \".\" ] #(1) words = [] current_word = \"\" #(2) for char in text : if char == \" \" : if not current_word == \"\" : words . append ( current_word ) current_word = \"\" #(3) elif char in delimiters : if current_word == \"\" : #(4) words . append ( char ) else : words . append ( current_word ) words . append ( char ) current_word = \"\" #(5) else : current_word += char #(6) print ( words ) initialize list of delimiters the current_word variable keeps track of the word currently being processed. check if the character is a whitespace and the current_word is not empty check if the character is one of the punctuation marks and there is nothing stored in the current_word yet check if the character is one of the punctuation marks and there is information stored in current_word check if the character is any other letter; that is, not specified as a delimiter and not a whitespace To solve the problem with cases like U.S.A. and U.K. which under the above preprocessor are splitted into ['U', '.', 'S', '.', 'A', '.'] and ['U', '.', 'K', '.'], we can use tokenizer. Info Tokenization is the process of word token identification or extraction from the running text. It is often the first step in text preprocessing. Whitespaces and punctuation marks often serve as reliable word separators; however, simple approaches are likely to run into exceptions like \u201cU.S.A.\u201d and similar. Tokenizers are NLP tools that are highly optimized for the task of word tokenization, and they may rely on carefully crafted regular expressions or may be trained using machine-learning algorithms. 3. Extract and normalize features Put all words to lowercase. 4. Train a classifier Choose machine learning model and train it on the preprocessed data. Split dataset into train and test before training (typically 80% for training, and 20% for testing). Na\u00efve Bayes - probabilistic classifier, it makes class prediction based on the estimation which outcome is most likely. The probability in this case is conditional , because it depends on condition (words as features). If P(spam | content) = 0.58 and P(ham | content) = 0.42, predict spam If P(spam | content) = 0.37 and P(ham | content) = 0.63, predict ham How it works on practice Estimate the probability that an email is spam or ham based on its content, taking the number of times this content lead to a particular outcome: \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{num(spam\\ emails\\ with\\ 'Participate\\ in\\ our\\ lottery!')}{num(all\\ email\\ with\\ 'Participate\\ in\\ our\\ lottery!')} \\] In general the formula looks like this: \\[ \\operatorname{P}(outcome\\ |{condition}) = \\frac{num\\ of\\ times(condition\\ led\\ to\\ outcome)}{num\\ of\\ times(condition\\ applied)} \\] The problem here is that we are estimating of particular combination of words (as features), but it may not be many times when we meet exactly the same combination. Therefore, we may not generalise the classifier efficiently. The solution is to split the estimation into smaller bits and establish the link between individual features. Estimate probability by using two formulas: Schema Math Formula 1 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all emails with this content. \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P('Participate\\ in\\ our\\ lottery!'\\ is\\ used\\ in\\ an\\ email)} \\] Formula 2 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all spam emails. \\[ \\operatorname{P}({'Participate\\ in\\ our\\ lottery!'|spam}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P(an\\ email\\ is\\ spam)} \\] General formula (combination of the two previous ones) : \\[ \\operatorname{P}({class|content}) = \\frac{P(content\\ in\\ class)}{P(content)} = \\frac{P(content|class)\\times P(class)}{P(content)} \\] Info P(class) - probability of each class, i.e. distribution of the class in the dataset. This forms prior probability for the classifier. If the dataset is unbalanced, potential probability is inclined respectively. P(content|class) - likelihood that we see this particular content given that the email is spam or ham. Na\u00efve Bayes assumes that the features are independent of each other, i.e. chances of seeing a word 'lottery' in an email are independent of seeing a word 'new' or any other word. Therefore we can estimate the probability of the whole sequence of features by product of probabilities of each feature in this class: \\[ \\operatorname{P}({['participate': True,\\ 'in': True,\\ ...,\\ '!': True]|spam}) = {P('participate': True|spam)\\times P('in': True|spam)\\times ...\\times P('!': True|spam)} \\] And general formula is: \\[ \\operatorname{P}({[f_1, f_2, ..., f_n]|class}) = {P(f_1|class)\\times P(f_2|class)\\times ...\\times P(f_n|class)} \\] How exactly classifier algorithm works Training Phase Test Phase Learns prior probabilities This is simply initial class distribution. For example: P(ham) = 0.71 P(spam) = 0.29 Learns probabilities for each feature given in the classes. This is the proportion of emails with each feature in each class. For example: P ('meeting': True|ham) = 0.50 Predict new email class based on the probabilities of each feature of the content: \\[ \\begin{align} \\begin{cases} P(f_1|spam)\\times ...\\times P(f_n|spam)\\times P(spam) \\geq P(f_1|ham)\\times ...\\times P(f_n|ham)\\times P(ham) \\implies spam\\\\ otherwise \\implies ham\\ \\end{cases} \\end{align} \\] Code implementation from nltk import NaiveBayesClassifier , classify def train ( features , proportion ): train_size = int ( len ( features ) * proportion ) #(1) train_set = features [: train_size ] test_set = features [ train_size :] print ( f \"Training set size = { str ( len ( train_set )) } emails\" ) print ( f \"Test set size = { str ( len ( test_set )) } emails\" ) classifier = NaiveBayesClassifier . train ( train_set ) #(2) return train_set , test_set , classifier train_set , test_set , classifier = train ( all_features , 0.8 ) #(3) split dataset into train and test parts initialize a classifier apply the train function using 80% of dataset for training 5. Classifier evaluation \\[ \\operatorname{Accuracy} = \\frac{num(correct\\ predictions)}{num(all\\ test\\ instances)} \\] Code implementation Define an estimation function which estimates the accuracy of the classifier on each dataset def evaluate ( train_set , test_set , classifier ): print ( f \"Accuracy on the training set = { str ( classify . accuracy ( classifier , train_set )) } \" ) print ( f \"Accuracy of the test set = { str ( classify . accuracy ( classifier , test_set )) } \" ) classifier . show_most_informative_features ( 50 ) #(1) evaluate ( train_set , test_set , classifier ) print top 50 most informative features (words). The words that are most strongly associated with a particular class. It is calculated as \\(max[P(word: True | ham) / P(word: True | spam)]\\) for most predictive ham features, and \\(max[P(word: True | spam) / P(word: True | ham)]\\) for most predictive spam features (check out NLTK\u2019s documentation for more information). Check the contexts of specific words from nltk.text import Text def concordance ( data_list , search_word ): for email in data_list : word_list = [ word for word in word_tokenize ( email . lower ())] text_list = Text ( word_list ) if search_word in word_list : text_list . concordance ( search_word ) print ( \"STOCKS in HAM:\" ) concordance ( ham_list , 'stocks' ) print ( \" \\n\\n STOCKS in SPAM:\" ) concordance ( spam_list , 'stocks' ) 6. Use spam filter on new emails in practice Here is the code for inference of the model on new emails. test_spam_list = [ \"Participate in our new lottery!\" , \"Try out this new medicine\" ] test_ham_list = [ \"See the minutes from the last meeting attached\" , \"Investors are coming to our office on Monday\" ] test_emails = [( email_content , \"spam\" ) for email_content in test_spam_list ] test_emails += [( email_content , \"ham\" ) for email_content in test_ham_list ] new_test_set = [( get_features ( email ), label ) for ( email , label ) in test_emails ] evaluate ( train_set , new_test_set , classifier ) while True : email = input ( \"Type in your email here (or press 'Enter'): \" ) if len ( email ) == 0 : break else : prediction = classifier . classify ( get_features ( email )) print ( f \"This email is likely { prediction } \\n \" )","title":"Suggested pipeline of the project"},{"location":"machine_learning/nlp/voice_cloning/","text":"Info Voice cloning , also known as text-to-speech synthesis or speech synthesis, is the process of creating a computer-generated version of a person's voice. This technology has many applications, including creating personalized voice assistants, generating voiceovers for videos, and creating text-to-speech systems for people with disabilities. There are two main approaches to voice cloning: concatenative synthesis and parametric synthesis. Concatenative Synthesis: Concatenative synthesis is a method of creating a synthetic voice by recording a large number of short speech segments and then stitching them together to form words and phrases. The recorded segments are carefully selected to capture the speaker's unique voice characteristics, such as pitch, rhythm, and tone. The result is a synthesized voice that sounds like the original speaker, but can produce any text that is entered into the system. The process of creating a synthetic voice using concatenative synthesis involves several steps: Data Collection : The first step is to collect a large amount of speech data from the target speaker. This can be done by recording the speaker reading a set of scripted sentences, or by using existing recordings of the speaker. Feature Extraction : Next, the speech data is analyzed to extract specific features that are unique to the speaker's voice. These features can include pitch, rhythm, and tone, as well as more subtle characteristics like vocal tics and accent. Speech Segmentation : The speech data is then segmented into small units of sound, typically around 10-30 milliseconds in length. These units are called \"phonemes\" and they represent the basic building blocks of speech. Alignment : The phonemes are then aligned with the corresponding text, so that the system knows which phoneme to use for each word. Concatenation : Finally, the system assembles the speech segments into the final synthesized voice, using sophisticated algorithms to smooth the transitions between phonemes and create a natural-sounding voice. Parametric Synthesis: Parametric synthesis involves training a machine learning model on a large amount of speech data, typically from multiple speakers, to learn the underlying patterns of speech production. The model is then used to generate new speech based on text input. The process of creating a synthetic voice using parametric synthesis involves several steps: Data Collection : The first step is to collect a large amount of speech data from multiple speakers. This can be done by recording speakers reading a set of scripted sentences, or by using existing recordings of speakers. Training the Model : Next, a deep neural network is trained on the speech data, using a technique called \"sequence-to-sequence learning.\" The model learns to map sequences of input text to sequences of speech output. Synthesizing Speech : Finally, the trained model can be used to generate new speech based on text input. The model is capable of generating speech in the style of any of the speakers used in the training data, as well as creating entirely new voices that have never been heard before. Parametric synthesis is more promissing approach and in detail it works as follows steps: Preprocessing : The speech data is preprocessed to extract useful features, such as the spectral envelope, the fundamental frequency (pitch), and the duration of each phoneme. Text Input : The model takes text input as its input, such as a sentence or a paragraph. Encoding : The text input is encoded into a sequence of vectors that represent the text. Synthesis : The model generates a sequence of vectors that represents the synthesized speech. This sequence of vectors is then transformed into an audio waveform using a signal processing technique called \"vocoder\". Training : The deep neural network is trained on the speech data and text input to learn the mapping between the two. This involves optimizing the model's parameters to minimize the difference between the synthesized speech and the actual speech. Examples of pre-trained voice cloning models: Tacotron 2 : This is a state-of-the-art text-to-speech model developed by Google that uses a combination of convolutional and recurrent neural networks to generate speech. The model has been trained on a large dataset of speech data and can generate speech that sounds natural and expressive. WaveNet : This is a neural network-based model for generating speech developed by DeepMind, a research company owned by Google. WaveNet is capable of generating high-quality speech that sounds very natural and human-like. . Deep Voice 3 : This is a text-to-speech synthesis model developed by Baidu Research that uses a deep neural network to generate speech. Deep Voice 3 is capable of generating speech that sounds very natural and expressive. Mozilla TTS : This is an open-source text-to-speech synthesis system developed by Mozilla that uses a deep neural network to generate speech. Mozilla TTS is capable of generating high-quality speech that sounds natural and expressive. Training Multilingual Model for Voice Cloning Choose library for TTS and STT I chose up Coqui TTS as it looked promising in terms of code usability, library support, open-source solutoin and number of pretrained models. It has good results regarding voice clonning for several languages, but not for Russian. The most relevant idea seemed to me to pick up multilingual Vits model and train it on RUSLAN training dataset Train model You can have a look at My Google Colab Notebook as a result of my training attempts. Another training notebook example can be found here - Kaggle training notebook on Kaggle. Results Unfortunately, Kaggle trainnig notebook resulted in trained model without speaker embedded. And my dozen of my training attempts were unlucky so far. The issues rised during model training need to be urther investigated.","title":"Voice Cloning"},{"location":"machine_learning/nlp/voice_cloning/#concatenative-synthesis","text":"Concatenative synthesis is a method of creating a synthetic voice by recording a large number of short speech segments and then stitching them together to form words and phrases. The recorded segments are carefully selected to capture the speaker's unique voice characteristics, such as pitch, rhythm, and tone. The result is a synthesized voice that sounds like the original speaker, but can produce any text that is entered into the system. The process of creating a synthetic voice using concatenative synthesis involves several steps: Data Collection : The first step is to collect a large amount of speech data from the target speaker. This can be done by recording the speaker reading a set of scripted sentences, or by using existing recordings of the speaker. Feature Extraction : Next, the speech data is analyzed to extract specific features that are unique to the speaker's voice. These features can include pitch, rhythm, and tone, as well as more subtle characteristics like vocal tics and accent. Speech Segmentation : The speech data is then segmented into small units of sound, typically around 10-30 milliseconds in length. These units are called \"phonemes\" and they represent the basic building blocks of speech. Alignment : The phonemes are then aligned with the corresponding text, so that the system knows which phoneme to use for each word. Concatenation : Finally, the system assembles the speech segments into the final synthesized voice, using sophisticated algorithms to smooth the transitions between phonemes and create a natural-sounding voice.","title":"Concatenative Synthesis:"},{"location":"machine_learning/nlp/voice_cloning/#parametric-synthesis","text":"Parametric synthesis involves training a machine learning model on a large amount of speech data, typically from multiple speakers, to learn the underlying patterns of speech production. The model is then used to generate new speech based on text input. The process of creating a synthetic voice using parametric synthesis involves several steps: Data Collection : The first step is to collect a large amount of speech data from multiple speakers. This can be done by recording speakers reading a set of scripted sentences, or by using existing recordings of speakers. Training the Model : Next, a deep neural network is trained on the speech data, using a technique called \"sequence-to-sequence learning.\" The model learns to map sequences of input text to sequences of speech output. Synthesizing Speech : Finally, the trained model can be used to generate new speech based on text input. The model is capable of generating speech in the style of any of the speakers used in the training data, as well as creating entirely new voices that have never been heard before. Parametric synthesis is more promissing approach and in detail it works as follows steps: Preprocessing : The speech data is preprocessed to extract useful features, such as the spectral envelope, the fundamental frequency (pitch), and the duration of each phoneme. Text Input : The model takes text input as its input, such as a sentence or a paragraph. Encoding : The text input is encoded into a sequence of vectors that represent the text. Synthesis : The model generates a sequence of vectors that represents the synthesized speech. This sequence of vectors is then transformed into an audio waveform using a signal processing technique called \"vocoder\". Training : The deep neural network is trained on the speech data and text input to learn the mapping between the two. This involves optimizing the model's parameters to minimize the difference between the synthesized speech and the actual speech. Examples of pre-trained voice cloning models: Tacotron 2 : This is a state-of-the-art text-to-speech model developed by Google that uses a combination of convolutional and recurrent neural networks to generate speech. The model has been trained on a large dataset of speech data and can generate speech that sounds natural and expressive. WaveNet : This is a neural network-based model for generating speech developed by DeepMind, a research company owned by Google. WaveNet is capable of generating high-quality speech that sounds very natural and human-like. . Deep Voice 3 : This is a text-to-speech synthesis model developed by Baidu Research that uses a deep neural network to generate speech. Deep Voice 3 is capable of generating speech that sounds very natural and expressive. Mozilla TTS : This is an open-source text-to-speech synthesis system developed by Mozilla that uses a deep neural network to generate speech. Mozilla TTS is capable of generating high-quality speech that sounds natural and expressive.","title":"Parametric Synthesis:"},{"location":"machine_learning/nlp/voice_cloning/#training-multilingual-model-for-voice-cloning","text":"Choose library for TTS and STT I chose up Coqui TTS as it looked promising in terms of code usability, library support, open-source solutoin and number of pretrained models. It has good results regarding voice clonning for several languages, but not for Russian. The most relevant idea seemed to me to pick up multilingual Vits model and train it on RUSLAN training dataset Train model You can have a look at My Google Colab Notebook as a result of my training attempts. Another training notebook example can be found here - Kaggle training notebook on Kaggle. Results Unfortunately, Kaggle trainnig notebook resulted in trained model without speaker embedded. And my dozen of my training attempts were unlucky so far. The issues rised during model training need to be urther investigated.","title":"Training Multilingual Model for Voice Cloning"},{"location":"projects/","text":"","title":"Projects"},{"location":"projects/age_classification/","text":"Summary The aim of this project is to make a research and find a solution for age classification based on face recognition. Based on face images collected from the camera, the ML model should classify faces into one of the specific age ranges. Sources Review The following articles were analysed: Gupta, S.K., Nain, N. Review: Single attribute and multi attribute facial gender and age estimation. Multimed Tools Appl (2022) ELKarazle, K.; Raman, V.; Then, P. Facial Age Estimation Using Machine Learning Techniques: An Overview. Big Data Cogn. Comput. 2022, 6, 128. Research Results The result of facial age estimation can be either age range (classification problem) or exact age value (regression problem). Main challenges in building efficient age prediction systems are: Real-life factors of face imaging such as resolution , sharpness , illumination , expression , occlusion , profile , frontal view , constraint environment , unconstraint environment , longitudinal , race , hair , scale , etc. Datasets creation Existing datasets usually divided into the following types: controlled (prepared in controlled environment with limited variability). Examples: FG-NET , MORPH , VADANA uncontrolled (prepared in real-life with different variability). Examples: LFW , IMDB-WIKI , ChaLearn Looking at People (LAP) , Specs on Face (SoF) , MSU LFW+ , YGA Steps for dataset creation: Image of faces collection Pre-processing cropping rotating aligning augmenting Evaluation metrics used: accuracy : \\((true positive + true negative)/(total test samples)\\) , and Mean Absolute Error (MAE) : mean value of the absolute differences between predicted age and real age (ground truth) of test samples Steps of model building 1. Feature extraction Extract unique and distinguishable patterns related to particular age classes. That includes facial age features such as texture or edge relathioships of facial skin. Global Features - apperance based feature extraction. The whole face is considered as feature space. Local Features - geometry based feature extraction. Feature extraction from facial parts as eyebrow, nose, lips, etc. 2. Building classification or regression model Handcrafted-based approach based on combination of filters: Histogram of Oriented Gradients (HOG) or Local Binary Pattern (LBP) to extract edges and shapes from facial image Deep learning approach based on CNNs. Examples of pre-trained models used for age estimation: VGG-16, VGG-19, ResNet50, AlexNet, Xception, GoogleLeNet, MobileNetV2","title":"Age Classification"},{"location":"projects/age_classification/#summary","text":"The aim of this project is to make a research and find a solution for age classification based on face recognition. Based on face images collected from the camera, the ML model should classify faces into one of the specific age ranges.","title":"Summary"},{"location":"projects/age_classification/#sources-review","text":"The following articles were analysed: Gupta, S.K., Nain, N. Review: Single attribute and multi attribute facial gender and age estimation. Multimed Tools Appl (2022) ELKarazle, K.; Raman, V.; Then, P. Facial Age Estimation Using Machine Learning Techniques: An Overview. Big Data Cogn. Comput. 2022, 6, 128.","title":"Sources Review"},{"location":"projects/age_classification/#research-results","text":"The result of facial age estimation can be either age range (classification problem) or exact age value (regression problem). Main challenges in building efficient age prediction systems are: Real-life factors of face imaging such as resolution , sharpness , illumination , expression , occlusion , profile , frontal view , constraint environment , unconstraint environment , longitudinal , race , hair , scale , etc. Datasets creation Existing datasets usually divided into the following types: controlled (prepared in controlled environment with limited variability). Examples: FG-NET , MORPH , VADANA uncontrolled (prepared in real-life with different variability). Examples: LFW , IMDB-WIKI , ChaLearn Looking at People (LAP) , Specs on Face (SoF) , MSU LFW+ , YGA Steps for dataset creation: Image of faces collection Pre-processing cropping rotating aligning augmenting Evaluation metrics used: accuracy : \\((true positive + true negative)/(total test samples)\\) , and Mean Absolute Error (MAE) : mean value of the absolute differences between predicted age and real age (ground truth) of test samples","title":"Research Results"},{"location":"projects/age_classification/#steps-of-model-building","text":"1. Feature extraction Extract unique and distinguishable patterns related to particular age classes. That includes facial age features such as texture or edge relathioships of facial skin. Global Features - apperance based feature extraction. The whole face is considered as feature space. Local Features - geometry based feature extraction. Feature extraction from facial parts as eyebrow, nose, lips, etc. 2. Building classification or regression model Handcrafted-based approach based on combination of filters: Histogram of Oriented Gradients (HOG) or Local Binary Pattern (LBP) to extract edges and shapes from facial image Deep learning approach based on CNNs. Examples of pre-trained models used for age estimation: VGG-16, VGG-19, ResNet50, AlexNet, Xception, GoogleLeNet, MobileNetV2","title":"Steps of model building"},{"location":"projects/book_summarizer/","text":"","title":"Book Summarizer"},{"location":"software_architecture/","text":"Software Architecture This section contains knowledge about software architecture concepts, patterns, and best practices. Due to my love of Python, architectural examples are provided in Python. Sources Overview Books Software Architecture for Busy Developers YouTube Videos Architecture of modern WEB applications. Evolution from A to Z Courses Educative.io Main Concepts Architecture - set of modules and components of the system, description of how these modules and components should be developed, how they should be connected, and interfaces that specify the purpose of each module and component. Components inside modules should be tightly-coupled . Modules should be loosely-coupled . Deletion/revision of a module should be simple, and should not affect other modules significantly. Separation of concerns - different parts of a system should manage different parts of the process. Inversion of control - helps to avoid strong coupling between components of the system. It realised by wrapping components and expose specific interface. Thus, components can be connected via interface rather than their hard-coded specific implementation. Think of the following responsibilities while architecting software: Address functional and non-functional requirements Use technical standards, coding best practices and design patterns Interact with stakeholders to ensure smooth development and implementation Play an active role in the development process Proactive watching over tech trends and paradigm shifts, but do not follow them blindly Main architectural principles DRY SOLID KISS PATTERNS Architecture of WEB Application Each component of the above schema is described in the respective topic of the Backend or Frontend sections. Architectural Styles Monolith benefits: easy to develop easy to deploy (as a single package) easy to monitor The key is simplicity, as long as the application remains lightweight. challenges: adding new features and debugging becomes complicated while the app grows the code is tightly coupled, so it violates single responsibility principle lack of granular scalability availability at risk (error in one module can cause of cascade blocking of the whole app) high risk of technical debt Service Oriented Architecture (SOA) It has middle core componet - Enterprise Service Bus benefits: decouple application and services better governance, as SOA aims to provide single source of truth reusability of components technology agnostic scalability, as each service is independent challenges: lack of agility","title":"Software Architecture"},{"location":"software_architecture/#software-architecture","text":"This section contains knowledge about software architecture concepts, patterns, and best practices. Due to my love of Python, architectural examples are provided in Python.","title":"Software Architecture"},{"location":"software_architecture/#sources-overview","text":"","title":"Sources Overview"},{"location":"software_architecture/#books","text":"Software Architecture for Busy Developers","title":"Books"},{"location":"software_architecture/#youtube-videos","text":"Architecture of modern WEB applications. Evolution from A to Z","title":"YouTube Videos"},{"location":"software_architecture/#courses","text":"Educative.io","title":"Courses"},{"location":"software_architecture/#main-concepts","text":"Architecture - set of modules and components of the system, description of how these modules and components should be developed, how they should be connected, and interfaces that specify the purpose of each module and component. Components inside modules should be tightly-coupled . Modules should be loosely-coupled . Deletion/revision of a module should be simple, and should not affect other modules significantly. Separation of concerns - different parts of a system should manage different parts of the process. Inversion of control - helps to avoid strong coupling between components of the system. It realised by wrapping components and expose specific interface. Thus, components can be connected via interface rather than their hard-coded specific implementation. Think of the following responsibilities while architecting software: Address functional and non-functional requirements Use technical standards, coding best practices and design patterns Interact with stakeholders to ensure smooth development and implementation Play an active role in the development process Proactive watching over tech trends and paradigm shifts, but do not follow them blindly Main architectural principles DRY SOLID KISS PATTERNS","title":"Main Concepts"},{"location":"software_architecture/#architecture-of-web-application","text":"Each component of the above schema is described in the respective topic of the Backend or Frontend sections.","title":"Architecture of WEB Application"},{"location":"software_architecture/#architectural-styles","text":"Monolith benefits: easy to develop easy to deploy (as a single package) easy to monitor The key is simplicity, as long as the application remains lightweight. challenges: adding new features and debugging becomes complicated while the app grows the code is tightly coupled, so it violates single responsibility principle lack of granular scalability availability at risk (error in one module can cause of cascade blocking of the whole app) high risk of technical debt Service Oriented Architecture (SOA) It has middle core componet - Enterprise Service Bus benefits: decouple application and services better governance, as SOA aims to provide single source of truth reusability of components technology agnostic scalability, as each service is independent challenges: lack of agility","title":"Architectural Styles"},{"location":"software_architecture/architectural_patterns/mv_patterns/","text":"MV* Architectural Patterns Brief description of the following patterns: MVC MVVM MVP Key Ideas Split business logic of the application from interface. Business logic is the core functionality that solves specific business problem ( for example, user registration, payments, add item to the basket, blog post creation, etc.) It is an algorithm of process that we want to implement to transform data somehow in order to provide a service (solve business problem). Sometimes it is called \"use case\". Interface is the graphical interface that user interacts with. Such as forms, buttons, etc. Controller is an intermediary between the UI (View) and business logic (Model). It should be 'thin', so it should not duplicate business logic or take any part of it, only some 'light' features, such as values validation.","title":"Architectural Patterns"},{"location":"software_architecture/architectural_patterns/mv_patterns/#mv-architectural-patterns","text":"Brief description of the following patterns: MVC MVVM MVP","title":"MV* Architectural Patterns"},{"location":"software_architecture/architectural_patterns/mv_patterns/#key-ideas","text":"Split business logic of the application from interface. Business logic is the core functionality that solves specific business problem ( for example, user registration, payments, add item to the basket, blog post creation, etc.) It is an algorithm of process that we want to implement to transform data somehow in order to provide a service (solve business problem). Sometimes it is called \"use case\". Interface is the graphical interface that user interacts with. Such as forms, buttons, etc. Controller is an intermediary between the UI (View) and business logic (Model). It should be 'thin', so it should not duplicate business logic or take any part of it, only some 'light' features, such as values validation.","title":"Key Ideas"}]}