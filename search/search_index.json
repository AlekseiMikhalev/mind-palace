{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This is my personal Mind Palace, where I collect knowledge from my learning journey. Every time I read some tech book, documentation, science article, or \"how to\" recipe, I make a summary in the respective section. After I watched a course or project I make a summary too. I took this approach to solve one simple problem - remember what I've read or learnt and be able to apply the knowledge in my life. Mind Palace Structure: Mind Palace consist of the following nested categories: Knowledge section (the widest knowledge group) Knowledge concept (category of the group related to the particular technology) Code snippets (examples of how to implement the particular knowledge concept) Code snippets also include the detailed explanation of each line of code. That helps me to understand how the code works and remember it later during the implementation. The knowledge base in this digital Mind Palace is related to various software development areas. My learning path is focused on expanding my knowledge step-by-step and eventually building a solid base for developing robust complex systems. Therefore, it includes knowledge from frontend and backend development, machine learning, data science, databases, software architecture, etc. Couple of words about me I am a self-taught software developer. I started my programming journey by heart. In my childhood I wanted to be a programmer. I took some gadgets (whether it was my grandfather's watch or music player from the town dump), disassemble them and tried to assemble again. I was extremely interested to understand how they work. But something went wrong and I became a lawyer. My lawyer career includes eight years in international law firms and venture funds as a corporate lawyer. Spending long hours on drafting documents, doing due diligence or revising prospectuses for capital market deals, I realised that it's not what I want to do for life. It is just not my passion. And in my spare time I started to learn programing. Step-by-step I switched into software engineering and left my lawyer career behind. That was, and actually still very hard... But you know what? I am happy! It gives me energy and meaningful start every day. And I won't give up Comments, recommendation and revisions I would be happy if you give me any useful comments, recommendation or point out on my mistakes or code errors.","title":"Introduction"},{"location":"#introduction","text":"This is my personal Mind Palace, where I collect knowledge from my learning journey. Every time I read some tech book, documentation, science article, or \"how to\" recipe, I make a summary in the respective section. After I watched a course or project I make a summary too. I took this approach to solve one simple problem - remember what I've read or learnt and be able to apply the knowledge in my life.","title":"Introduction"},{"location":"#mind-palace-structure","text":"Mind Palace consist of the following nested categories: Knowledge section (the widest knowledge group) Knowledge concept (category of the group related to the particular technology) Code snippets (examples of how to implement the particular knowledge concept) Code snippets also include the detailed explanation of each line of code. That helps me to understand how the code works and remember it later during the implementation. The knowledge base in this digital Mind Palace is related to various software development areas. My learning path is focused on expanding my knowledge step-by-step and eventually building a solid base for developing robust complex systems. Therefore, it includes knowledge from frontend and backend development, machine learning, data science, databases, software architecture, etc.","title":"Mind Palace Structure:"},{"location":"#couple-of-words-about-me","text":"I am a self-taught software developer. I started my programming journey by heart. In my childhood I wanted to be a programmer. I took some gadgets (whether it was my grandfather's watch or music player from the town dump), disassemble them and tried to assemble again. I was extremely interested to understand how they work. But something went wrong and I became a lawyer. My lawyer career includes eight years in international law firms and venture funds as a corporate lawyer. Spending long hours on drafting documents, doing due diligence or revising prospectuses for capital market deals, I realised that it's not what I want to do for life. It is just not my passion. And in my spare time I started to learn programing. Step-by-step I switched into software engineering and left my lawyer career behind. That was, and actually still very hard... But you know what? I am happy! It gives me energy and meaningful start every day. And I won't give up","title":"Couple of words about me"},{"location":"#comments-recommendation-and-revisions","text":"I would be happy if you give me any useful comments, recommendation or point out on my mistakes or code errors.","title":"Comments, recommendation and revisions"},{"location":"backend/","text":"General Knowledge about Backend This section contains information about general backend knowledge. More specific knowledge are collected into the more narrowed sections.","title":"Backend"},{"location":"backend/#general-knowledge-about-backend","text":"This section contains information about general backend knowledge. More specific knowledge are collected into the more narrowed sections.","title":"General Knowledge about Backend"},{"location":"backend/design_patterns/","text":"Programming Design Patterns in Python Profiling Profilers in Python Profiling - technique that allows to pinpoint the most resource-intensive parts of an application. Profiler - program that runs an aplication and monitors how long each function takes to execute, so it detects the functions on which our application spends of its time. pytest-benchmark is used for testing running time of specific functions. See documentation of pytest-benchmark for details. Another profiling tool is cProfile . from simul import benchmark import cProfile pr = cProfile . Profile () pr . enable () benchmark () pr . disable () pr . print_stats () KCachegrind - graphical user interface visualizing profiling output. How to do profiling: Check modules of the app with cProfile or other profiler in order to understand which module is time-consuming Use line_profiler to check functions line by line. See the documentation Profiling memory usage Memory profiler summarizes the information of memory usage of process. Optimizing code Methods of optimzation: improve algorithms used minimize the number of instructions Using the right data structures Estimation time of operations per data sctructure: list of 10,000 size deque (double-ended queue) of 10,000 size Tip A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the bisect module . bisect allows fast search on sorted arrays. insert bisect collection = [ 1 , 2 , 4 , 5 , 6 ] bisect . bisect ( collection , 3 ) # Result: 2 This function uses the binary search algorithm that has \\(O(log(N))\\) running time. The efficiency of the bisect function: dictionaries The efficiency of the Counter function: Building an in-memory search index using a hash map docs = [ \"the cat is under the table\" , \"the dog is under the table\" , \"cats and dogs smell roses\" , \"Carla eats an apple\" ] index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : #(3) index [ word ] = [ i ] else : index [ word ] . append ( i ) Building an index We iterate over each term in the document We build a list containing the indices where the term appears Now if we want to do search (for example we want to retrieve all documents contain the table term), we can easily use it to query the index: results = index [ \"table\" ] result_documents = [ docs [ i ] for i in results ] And it takes us \\(O(1)\\) time complexity. So we can query any number of documents in constant time. sets Unordered collections of unique elements. Time complexity of union , intersection and difference operations of two sets. Application of sets in practice: boolean search - for example, query all documents contain multiple terms. For example, we may want to search for all the documents that contain the words cat and table . This kind of query can be efficiently computed by taking the intersection between the set of documents containing cat and the set of documents containing table. index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : index [ word ] = { i } #(3) else : index [ word ] . add ( i ) index [ 'cat' ] . intersection ( index [ 'table' ]) #(4) Building an index using sets We iterate over each term in the document We build a set containing the indices where the term appears Querying the documents containing both \"cat\" and \"table\" heaps Heaps are data structures designed to quickly fin and extract the maximum (or minimum) value in a collection. Application of heaps in practice: process of incoming tasks in order of maximum priority. Insertion or extraction of maximum value takes \\(O(log(N))\\) time complexity. import heapq collection = [ 10 , 3 , 3 , 4 , 5 , 6 ] heapq . heapify ( collection ) heapq . heappop ( collection ) #(1) extract the minimum value, returns: 3 Another example of performing insertion and extraction from queue import PriorityQueue queue = PriorityQueue () for element in collection : queue . put ( element ) queue . get () #(1) returns: 3 If the maximum element is required we can just multiply each element of the list by -1, i.e. invert the order of elements. If it is needed to associate an object (for example, task) with each number (and make priority order) we can insert tuples (number, object) : queue = PriorityQueue () queue . put (( 3 , \"priority 3\" )) queue . put (( 2 , \"priority 2\" )) queue . put (( 1 , \"priority 1\" )) queue . get () #(1) returns: (1, \"priority 1\") tries Tries are extremely fast at matching a list of strings against a prefix. This is especially useful when implementing features such as search as you type and autocompletion , where the list of available completions is very large and short response times are required. from patricia import trie from random import choice from string import ascii_uppercase def random_string ( length ): \"\"\"Produce a random string made of *length* uppercase \\ ascii characters\"\"\" return '' . join ( choice ( ascii_uppercase ) for i in \\ range ( length )) strings = [ random_string ( 32 ) for i in range ( 10000 )] strings_dict = { s : 0 for s in strings } #(1) strings_trie = trie ( ** strings_dict ) matches = list ( strings_trie . iter ( 'AA' )) a dictionary where all values are 0 Using caching and memorization to improve efficiency The idea behind caching is to store expensive results in a temporary location ( cache ) that can be located in memory, on disk, or in a remote location.","title":"Design Patterns"},{"location":"backend/design_patterns/#programming-design-patterns-in-python","text":"","title":"Programming Design Patterns in Python"},{"location":"backend/design_patterns/#profiling","text":"Profilers in Python Profiling - technique that allows to pinpoint the most resource-intensive parts of an application. Profiler - program that runs an aplication and monitors how long each function takes to execute, so it detects the functions on which our application spends of its time. pytest-benchmark is used for testing running time of specific functions. See documentation of pytest-benchmark for details. Another profiling tool is cProfile . from simul import benchmark import cProfile pr = cProfile . Profile () pr . enable () benchmark () pr . disable () pr . print_stats () KCachegrind - graphical user interface visualizing profiling output. How to do profiling: Check modules of the app with cProfile or other profiler in order to understand which module is time-consuming Use line_profiler to check functions line by line. See the documentation","title":"Profiling"},{"location":"backend/design_patterns/#profiling-memory-usage","text":"Memory profiler summarizes the information of memory usage of process.","title":"Profiling memory usage"},{"location":"backend/design_patterns/#optimizing-code","text":"Methods of optimzation: improve algorithms used minimize the number of instructions","title":"Optimizing code"},{"location":"backend/design_patterns/#using-the-right-data-structures","text":"Estimation time of operations per data sctructure: list of 10,000 size deque (double-ended queue) of 10,000 size Tip A simple way to speed up searches in lists is to keep the array sorted and perform a binary search using the bisect module . bisect allows fast search on sorted arrays. insert bisect collection = [ 1 , 2 , 4 , 5 , 6 ] bisect . bisect ( collection , 3 ) # Result: 2 This function uses the binary search algorithm that has \\(O(log(N))\\) running time. The efficiency of the bisect function: dictionaries The efficiency of the Counter function: Building an in-memory search index using a hash map docs = [ \"the cat is under the table\" , \"the dog is under the table\" , \"cats and dogs smell roses\" , \"Carla eats an apple\" ] index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : #(3) index [ word ] = [ i ] else : index [ word ] . append ( i ) Building an index We iterate over each term in the document We build a list containing the indices where the term appears Now if we want to do search (for example we want to retrieve all documents contain the table term), we can easily use it to query the index: results = index [ \"table\" ] result_documents = [ docs [ i ] for i in results ] And it takes us \\(O(1)\\) time complexity. So we can query any number of documents in constant time. sets Unordered collections of unique elements. Time complexity of union , intersection and difference operations of two sets. Application of sets in practice: boolean search - for example, query all documents contain multiple terms. For example, we may want to search for all the documents that contain the words cat and table . This kind of query can be efficiently computed by taking the intersection between the set of documents containing cat and the set of documents containing table. index = {} #(1) for i , doc in enumerate ( docs ): for word in doc . split (): #(2) if word not in index : index [ word ] = { i } #(3) else : index [ word ] . add ( i ) index [ 'cat' ] . intersection ( index [ 'table' ]) #(4) Building an index using sets We iterate over each term in the document We build a set containing the indices where the term appears Querying the documents containing both \"cat\" and \"table\" heaps Heaps are data structures designed to quickly fin and extract the maximum (or minimum) value in a collection. Application of heaps in practice: process of incoming tasks in order of maximum priority. Insertion or extraction of maximum value takes \\(O(log(N))\\) time complexity. import heapq collection = [ 10 , 3 , 3 , 4 , 5 , 6 ] heapq . heapify ( collection ) heapq . heappop ( collection ) #(1) extract the minimum value, returns: 3 Another example of performing insertion and extraction from queue import PriorityQueue queue = PriorityQueue () for element in collection : queue . put ( element ) queue . get () #(1) returns: 3 If the maximum element is required we can just multiply each element of the list by -1, i.e. invert the order of elements. If it is needed to associate an object (for example, task) with each number (and make priority order) we can insert tuples (number, object) : queue = PriorityQueue () queue . put (( 3 , \"priority 3\" )) queue . put (( 2 , \"priority 2\" )) queue . put (( 1 , \"priority 1\" )) queue . get () #(1) returns: (1, \"priority 1\") tries Tries are extremely fast at matching a list of strings against a prefix. This is especially useful when implementing features such as search as you type and autocompletion , where the list of available completions is very large and short response times are required. from patricia import trie from random import choice from string import ascii_uppercase def random_string ( length ): \"\"\"Produce a random string made of *length* uppercase \\ ascii characters\"\"\" return '' . join ( choice ( ascii_uppercase ) for i in \\ range ( length )) strings = [ random_string ( 32 ) for i in range ( 10000 )] strings_dict = { s : 0 for s in strings } #(1) strings_trie = trie ( ** strings_dict ) matches = list ( strings_trie . iter ( 'AA' )) a dictionary where all values are 0","title":"Using the right data structures"},{"location":"backend/design_patterns/#using-caching-and-memorization-to-improve-efficiency","text":"The idea behind caching is to store expensive results in a temporary location ( cache ) that can be located in memory, on disk, or in a remote location.","title":"Using caching and memorization to improve efficiency"},{"location":"frontend/","text":"","title":"Frontend"},{"location":"machine_learning/","text":"","title":"Machine Learning"},{"location":"machine_learning/nlp/","text":"Natural Language Processing Sources Overview Books: Ekaterina Kochmar - Getting Started with Natural Language Processing-Manning (2022) Sample Use Cases Information search Information retrieval Basics In NLP important concept is a vector. Characters can be represented as vectors, words, or documents can be represented as vectors. For example, two documents can be represented as vectors based on the number of occurences of specific words. Example Schema Python Implementation doc1 = \"meeting ... management ... meeting ... management ... meeting\" # (1) doc1 += \"... management ... meeting ... meeting\" vector = [ 0 , 0 ] # (2) for word in doc1 . split ( \" \" ): if word == \"management\" : vector [ 0 ] = vector [ 0 ] + 1 # (3) if word == \"meeting\" : vector [ 1 ] = vector [ 1 ] + 1 # (4) print ( vector ) # (5) represents a document based on keywords only initializes array vector count for \"management\" is incremented in cell 0 count for \"meeting\" is incremented in cell 1. [3, 5] Two vectors can be compared between each other by either length or by measuring angle between them. Info Cosine similarity estimates the similarity between two nonzero vectors in space (or two texts represented by such vectors) on the basis of the angle between these vectors \u2014 for example, the cosine of 0\u00b0 equals 1, which denotes the maximum similarity, and the cosine of 180\u00b0 equals \u20131, which is the lowest value. Unlike Euclidean distance, this measure is not affected by vector length.","title":"NLP"},{"location":"machine_learning/nlp/#natural-language-processing","text":"","title":"Natural Language Processing"},{"location":"machine_learning/nlp/#sources-overview","text":"Books: Ekaterina Kochmar - Getting Started with Natural Language Processing-Manning (2022)","title":"Sources Overview"},{"location":"machine_learning/nlp/#sample-use-cases","text":"Information search Information retrieval","title":"Sample Use Cases"},{"location":"machine_learning/nlp/#basics","text":"In NLP important concept is a vector. Characters can be represented as vectors, words, or documents can be represented as vectors. For example, two documents can be represented as vectors based on the number of occurences of specific words. Example Schema Python Implementation doc1 = \"meeting ... management ... meeting ... management ... meeting\" # (1) doc1 += \"... management ... meeting ... meeting\" vector = [ 0 , 0 ] # (2) for word in doc1 . split ( \" \" ): if word == \"management\" : vector [ 0 ] = vector [ 0 ] + 1 # (3) if word == \"meeting\" : vector [ 1 ] = vector [ 1 ] + 1 # (4) print ( vector ) # (5) represents a document based on keywords only initializes array vector count for \"management\" is incremented in cell 0 count for \"meeting\" is incremented in cell 1. [3, 5] Two vectors can be compared between each other by either length or by measuring angle between them. Info Cosine similarity estimates the similarity between two nonzero vectors in space (or two texts represented by such vectors) on the basis of the angle between these vectors \u2014 for example, the cosine of 0\u00b0 equals 1, which denotes the maximum similarity, and the cosine of 180\u00b0 equals \u20131, which is the lowest value. Unlike Euclidean distance, this measure is not affected by vector length.","title":"Basics"},{"location":"machine_learning/nlp/information_search/","text":"Information Search / Information Retrieval This section explains search algorithm from beginning to end. Understanding the Purpose of the Project Information search algorithm in a nutshel: Boolean search algorithm It is simple search algo that selects all documents that contain any of the words from the query Preprocessing step Preprocessing step is needed to remove words and punctuation which does not contain any useful information for the purpose of our search algorithm. import nltk import string nltk . download ( 'stopwords' ) from nltk import word_tokenize from nltk.corpus import stopwords def process ( text ): stoplist = set ( stopwords . words ( 'english' )) word_list = [ word for word in word_tokenize ( text . lower ()) \\ if not word in stoplist and not word in string . punctuation ] return word_list word_list = process ( documents . get ( \"1\" )) print ( word_list )","title":"Information Search"},{"location":"machine_learning/nlp/information_search/#information-search-information-retrieval","text":"This section explains search algorithm from beginning to end.","title":"Information Search / Information Retrieval"},{"location":"machine_learning/nlp/information_search/#understanding-the-purpose-of-the-project","text":"Information search algorithm in a nutshel: Boolean search algorithm It is simple search algo that selects all documents that contain any of the words from the query Preprocessing step Preprocessing step is needed to remove words and punctuation which does not contain any useful information for the purpose of our search algorithm. import nltk import string nltk . download ( 'stopwords' ) from nltk import word_tokenize from nltk.corpus import stopwords def process ( text ): stoplist = set ( stopwords . words ( 'english' )) word_list = [ word for word in word_tokenize ( text . lower ()) \\ if not word in stoplist and not word in string . punctuation ] return word_list word_list = process ( documents . get ( \"1\" )) print ( word_list )","title":"Understanding the Purpose of the Project"},{"location":"machine_learning/nlp/text_classification/","text":"Text Classification In this section the knowledge about text classification is summarized based on the spam filtering project example. Info Classification refers to the process of identifying which category or class among the set of categories (classes) an observation belongs to based on its properties. In machine learning, such properties are called features and the class names are called class labels . If you classify observations into two classes, you are dealing with binary classification ; tasks with more than two classes are examples of multi-class classification . Understanding the Purpose of the Project Provided by client : dataset of spam and normal emails from the past. Task : build a spam filter which can predict if any future incoming email is spam or not. Questions to think about What is the format of provided data? How to use provided data? What features of the emails might be useful, how to extract them? What are sequence of steps in the application? Suggested pipeline of the project Define classes Split text of each email into words Extract useful features Train a classifier Test and evaluate 1. Define classes Define the label of each class and which data represents which class. 2. Split into words Preprocess the raw text from email in order to prepare it for machine learning classifier. Split text into words by whitespaces and punctuation: text = 'Define which data represents \"ham\" class and which data represents \"spam\" class for the machine learning algorithm.' delimiters = [ '\"' , \".\" ] #(1) words = [] current_word = \"\" #(2) for char in text : if char == \" \" : if not current_word == \"\" : words . append ( current_word ) current_word = \"\" #(3) elif char in delimiters : if current_word == \"\" : #(4) words . append ( char ) else : words . append ( current_word ) words . append ( char ) current_word = \"\" #(5) else : current_word += char #(6) print ( words ) initialize list of delimiters the current_word variable keeps track of the word currently being processed. check if the character is a whitespace and the current_word is not empty check if the character is one of the punctuation marks and there is nothing stored in the current_word yet check if the character is one of the punctuation marks and there is information stored in current_word check if the character is any other letter; that is, not specified as a delimiter and not a whitespace To solve the problem with cases like U.S.A. and U.K. which under the above preprocessor are splitted into ['U', '.', 'S', '.', 'A', '.'] and ['U', '.', 'K', '.'], we can use tokenizer. Info Tokenization is the process of word token identification or extraction from the running text. It is often the first step in text preprocessing. Whitespaces and punctuation marks often serve as reliable word separators; however, simple approaches are likely to run into exceptions like \u201cU.S.A.\u201d and similar. Tokenizers are NLP tools that are highly optimized for the task of word tokenization, and they may rely on carefully crafted regular expressions or may be trained using machine-learning algorithms. 3. Extract and normalize features Put all words to lowercase. 4. Train a classifier Choose machine learning model and train it on the preprocessed data. Split dataset into train and test before training (typically 80% for training, and 20% for testing). Na\u00efve Bayes - probabilistic classifier, it makes class prediction based on the estimation which outcome is most likely. The probability in this case is conditional , because it depends on condition (words as features). If P(spam | content) = 0.58 and P(ham | content) = 0.42, predict spam If P(spam | content) = 0.37 and P(ham | content) = 0.63, predict ham How it works on practice Estimate the probability that an email is spam or ham based on its content, taking the number of times this content lead to a particular outcome: \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{num(spam\\ emails\\ with\\ 'Participate\\ in\\ our\\ lottery!')}{num(all\\ email\\ with\\ 'Participate\\ in\\ our\\ lottery!')} \\] In general the formula looks like this: \\[ \\operatorname{P}(outcome\\ |{condition}) = \\frac{num\\ of\\ times(condition\\ led\\ to\\ outcome)}{num\\ of\\ times(condition\\ applied)} \\] The problem here is that we are estimating of particular combination of words (as features), but it may not be many times when we meet exactly the same combination. Therefore, we may not generalise the classifier efficiently. The solution is to split the estimation into smaller bits and establish the link between individual features. Estimate probability by using two formulas: Schema Math Formula 1 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all emails with this content. \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P('Participate\\ in\\ our\\ lottery!'\\ is\\ used\\ in\\ an\\ email)} \\] Formula 2 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all spam emails. \\[ \\operatorname{P}({'Participate\\ in\\ our\\ lottery!'|spam}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P(an\\ email\\ is\\ spam)} \\] General formula (combination of the two previous ones) : \\[ \\operatorname{P}({class|content}) = \\frac{P(content\\ in\\ class)}{P(content)} = \\frac{P(content|class)\\times P(class)}{P(content)} \\] Info P(class) - probability of each class, i.e. distribution of the class in the dataset. This forms prior probability for the classifier. If the dataset is unbalanced, potential probability is inclined respectively. P(content|class) - likelihood that we see this particular content given that the email is spam or ham. Na\u00efve Bayes assumes that the features are independent of each other, i.e. chances of seeing a word 'lottery' in an email are independent of seeing a word 'new' or any other word. Therefore we can estimate the probability of the whole sequence of features by product of probabilities of each feature in this class: \\[ \\operatorname{P}({['participate': True,\\ 'in': True,\\ ...,\\ '!': True]|spam}) = {P('participate': True|spam)\\times P('in': True|spam)\\times ...\\times P('!': True|spam)} \\] And general formula is: \\[ \\operatorname{P}({[f_1, f_2, ..., f_n]|class}) = {P(f_1|class)\\times P(f_2|class)\\times ...\\times P(f_n|class)} \\] How exactly classifier algorithm works Training Phase Test Phase Learns prior probabilities This is simply initial class distribution. For example: P(ham) = 0.71 P(spam) = 0.29 Learns probabilities for each feature given in the classes. This is the proportion of emails with each feature in each class. For example: P ('meeting': True|ham) = 0.50 Predict new email class based on the probabilities of each feature of the content: \\[ \\begin{align} \\begin{cases} P(f_1|spam)\\times ...\\times P(f_n|spam)\\times P(spam) \\geq P(f_1|ham)\\times ...\\times P(f_n|ham)\\times P(ham) \\implies spam\\\\ otherwise \\implies ham\\ \\end{cases} \\end{align} \\] Code implementation from nltk import NaiveBayesClassifier , classify def train ( features , proportion ): train_size = int ( len ( features ) * proportion ) #(1) train_set = features [: train_size ] test_set = features [ train_size :] print ( f \"Training set size = { str ( len ( train_set )) } emails\" ) print ( f \"Test set size = { str ( len ( test_set )) } emails\" ) classifier = NaiveBayesClassifier . train ( train_set ) #(2) return train_set , test_set , classifier train_set , test_set , classifier = train ( all_features , 0.8 ) #(3) split dataset into train and test parts initialize a classifier apply the train function using 80% of dataset for training 5. Classifier evaluation \\[ \\operatorname{Accuracy} = \\frac{num(correct\\ predictions)}{num(all\\ test\\ instances)} \\] Code implementation Define an estimation function which estimates the accuracy of the classifier on each dataset def evaluate ( train_set , test_set , classifier ): print ( f \"Accuracy on the training set = { str ( classify . accuracy ( classifier , train_set )) } \" ) print ( f \"Accuracy of the test set = { str ( classify . accuracy ( classifier , test_set )) } \" ) classifier . show_most_informative_features ( 50 ) #(1) evaluate ( train_set , test_set , classifier ) print top 50 most informative features (words). The words that are most strongly associated with a particular class. It is calculated as \\(max[P(word: True | ham) / P(word: True | spam)]\\) for most predictive ham features, and \\(max[P(word: True | spam) / P(word: True | ham)]\\) for most predictive spam features (check out NLTK\u2019s documentation for more information). Check the contexts of specific words from nltk.text import Text def concordance ( data_list , search_word ): for email in data_list : word_list = [ word for word in word_tokenize ( email . lower ())] text_list = Text ( word_list ) if search_word in word_list : text_list . concordance ( search_word ) print ( \"STOCKS in HAM:\" ) concordance ( ham_list , 'stocks' ) print ( \" \\n\\n STOCKS in SPAM:\" ) concordance ( spam_list , 'stocks' ) 6. Use spam filter on new emails in practice Here is the code for inference of the model on new emails. test_spam_list = [ \"Participate in our new lottery!\" , \"Try out this new medicine\" ] test_ham_list = [ \"See the minutes from the last meeting attached\" , \"Investors are coming to our office on Monday\" ] test_emails = [( email_content , \"spam\" ) for email_content in test_spam_list ] test_emails += [( email_content , \"ham\" ) for email_content in test_ham_list ] new_test_set = [( get_features ( email ), label ) for ( email , label ) in test_emails ] evaluate ( train_set , new_test_set , classifier ) while True : email = input ( \"Type in your email here (or press 'Enter'): \" ) if len ( email ) == 0 : break else : prediction = classifier . classify ( get_features ( email )) print ( f \"This email is likely { prediction } \\n \" )","title":"Text Classification"},{"location":"machine_learning/nlp/text_classification/#text-classification","text":"In this section the knowledge about text classification is summarized based on the spam filtering project example. Info Classification refers to the process of identifying which category or class among the set of categories (classes) an observation belongs to based on its properties. In machine learning, such properties are called features and the class names are called class labels . If you classify observations into two classes, you are dealing with binary classification ; tasks with more than two classes are examples of multi-class classification .","title":"Text Classification"},{"location":"machine_learning/nlp/text_classification/#understanding-the-purpose-of-the-project","text":"Provided by client : dataset of spam and normal emails from the past. Task : build a spam filter which can predict if any future incoming email is spam or not.","title":"Understanding the Purpose of the Project"},{"location":"machine_learning/nlp/text_classification/#questions-to-think-about","text":"What is the format of provided data? How to use provided data? What features of the emails might be useful, how to extract them? What are sequence of steps in the application?","title":"Questions to think about"},{"location":"machine_learning/nlp/text_classification/#suggested-pipeline-of-the-project","text":"Define classes Split text of each email into words Extract useful features Train a classifier Test and evaluate 1. Define classes Define the label of each class and which data represents which class. 2. Split into words Preprocess the raw text from email in order to prepare it for machine learning classifier. Split text into words by whitespaces and punctuation: text = 'Define which data represents \"ham\" class and which data represents \"spam\" class for the machine learning algorithm.' delimiters = [ '\"' , \".\" ] #(1) words = [] current_word = \"\" #(2) for char in text : if char == \" \" : if not current_word == \"\" : words . append ( current_word ) current_word = \"\" #(3) elif char in delimiters : if current_word == \"\" : #(4) words . append ( char ) else : words . append ( current_word ) words . append ( char ) current_word = \"\" #(5) else : current_word += char #(6) print ( words ) initialize list of delimiters the current_word variable keeps track of the word currently being processed. check if the character is a whitespace and the current_word is not empty check if the character is one of the punctuation marks and there is nothing stored in the current_word yet check if the character is one of the punctuation marks and there is information stored in current_word check if the character is any other letter; that is, not specified as a delimiter and not a whitespace To solve the problem with cases like U.S.A. and U.K. which under the above preprocessor are splitted into ['U', '.', 'S', '.', 'A', '.'] and ['U', '.', 'K', '.'], we can use tokenizer. Info Tokenization is the process of word token identification or extraction from the running text. It is often the first step in text preprocessing. Whitespaces and punctuation marks often serve as reliable word separators; however, simple approaches are likely to run into exceptions like \u201cU.S.A.\u201d and similar. Tokenizers are NLP tools that are highly optimized for the task of word tokenization, and they may rely on carefully crafted regular expressions or may be trained using machine-learning algorithms. 3. Extract and normalize features Put all words to lowercase. 4. Train a classifier Choose machine learning model and train it on the preprocessed data. Split dataset into train and test before training (typically 80% for training, and 20% for testing). Na\u00efve Bayes - probabilistic classifier, it makes class prediction based on the estimation which outcome is most likely. The probability in this case is conditional , because it depends on condition (words as features). If P(spam | content) = 0.58 and P(ham | content) = 0.42, predict spam If P(spam | content) = 0.37 and P(ham | content) = 0.63, predict ham How it works on practice Estimate the probability that an email is spam or ham based on its content, taking the number of times this content lead to a particular outcome: \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{num(spam\\ emails\\ with\\ 'Participate\\ in\\ our\\ lottery!')}{num(all\\ email\\ with\\ 'Participate\\ in\\ our\\ lottery!')} \\] In general the formula looks like this: \\[ \\operatorname{P}(outcome\\ |{condition}) = \\frac{num\\ of\\ times(condition\\ led\\ to\\ outcome)}{num\\ of\\ times(condition\\ applied)} \\] The problem here is that we are estimating of particular combination of words (as features), but it may not be many times when we meet exactly the same combination. Therefore, we may not generalise the classifier efficiently. The solution is to split the estimation into smaller bits and establish the link between individual features. Estimate probability by using two formulas: Schema Math Formula 1 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all emails with this content. \\[ \\operatorname{P}(spam\\ |{'Participate\\ in\\ our\\ lottery!'}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P('Participate\\ in\\ our\\ lottery!'\\ is\\ used\\ in\\ an\\ email)} \\] Formula 2 . Proportion of times we've seen 'Participate in our lottery!' in a spam email among all spam emails. \\[ \\operatorname{P}({'Participate\\ in\\ our\\ lottery!'|spam}) = \\frac{P('Participate\\ in\\ our\\ lottery!'\\ is\\ in\\ spam\\ email)}{P(an\\ email\\ is\\ spam)} \\] General formula (combination of the two previous ones) : \\[ \\operatorname{P}({class|content}) = \\frac{P(content\\ in\\ class)}{P(content)} = \\frac{P(content|class)\\times P(class)}{P(content)} \\] Info P(class) - probability of each class, i.e. distribution of the class in the dataset. This forms prior probability for the classifier. If the dataset is unbalanced, potential probability is inclined respectively. P(content|class) - likelihood that we see this particular content given that the email is spam or ham. Na\u00efve Bayes assumes that the features are independent of each other, i.e. chances of seeing a word 'lottery' in an email are independent of seeing a word 'new' or any other word. Therefore we can estimate the probability of the whole sequence of features by product of probabilities of each feature in this class: \\[ \\operatorname{P}({['participate': True,\\ 'in': True,\\ ...,\\ '!': True]|spam}) = {P('participate': True|spam)\\times P('in': True|spam)\\times ...\\times P('!': True|spam)} \\] And general formula is: \\[ \\operatorname{P}({[f_1, f_2, ..., f_n]|class}) = {P(f_1|class)\\times P(f_2|class)\\times ...\\times P(f_n|class)} \\] How exactly classifier algorithm works Training Phase Test Phase Learns prior probabilities This is simply initial class distribution. For example: P(ham) = 0.71 P(spam) = 0.29 Learns probabilities for each feature given in the classes. This is the proportion of emails with each feature in each class. For example: P ('meeting': True|ham) = 0.50 Predict new email class based on the probabilities of each feature of the content: \\[ \\begin{align} \\begin{cases} P(f_1|spam)\\times ...\\times P(f_n|spam)\\times P(spam) \\geq P(f_1|ham)\\times ...\\times P(f_n|ham)\\times P(ham) \\implies spam\\\\ otherwise \\implies ham\\ \\end{cases} \\end{align} \\] Code implementation from nltk import NaiveBayesClassifier , classify def train ( features , proportion ): train_size = int ( len ( features ) * proportion ) #(1) train_set = features [: train_size ] test_set = features [ train_size :] print ( f \"Training set size = { str ( len ( train_set )) } emails\" ) print ( f \"Test set size = { str ( len ( test_set )) } emails\" ) classifier = NaiveBayesClassifier . train ( train_set ) #(2) return train_set , test_set , classifier train_set , test_set , classifier = train ( all_features , 0.8 ) #(3) split dataset into train and test parts initialize a classifier apply the train function using 80% of dataset for training 5. Classifier evaluation \\[ \\operatorname{Accuracy} = \\frac{num(correct\\ predictions)}{num(all\\ test\\ instances)} \\] Code implementation Define an estimation function which estimates the accuracy of the classifier on each dataset def evaluate ( train_set , test_set , classifier ): print ( f \"Accuracy on the training set = { str ( classify . accuracy ( classifier , train_set )) } \" ) print ( f \"Accuracy of the test set = { str ( classify . accuracy ( classifier , test_set )) } \" ) classifier . show_most_informative_features ( 50 ) #(1) evaluate ( train_set , test_set , classifier ) print top 50 most informative features (words). The words that are most strongly associated with a particular class. It is calculated as \\(max[P(word: True | ham) / P(word: True | spam)]\\) for most predictive ham features, and \\(max[P(word: True | spam) / P(word: True | ham)]\\) for most predictive spam features (check out NLTK\u2019s documentation for more information). Check the contexts of specific words from nltk.text import Text def concordance ( data_list , search_word ): for email in data_list : word_list = [ word for word in word_tokenize ( email . lower ())] text_list = Text ( word_list ) if search_word in word_list : text_list . concordance ( search_word ) print ( \"STOCKS in HAM:\" ) concordance ( ham_list , 'stocks' ) print ( \" \\n\\n STOCKS in SPAM:\" ) concordance ( spam_list , 'stocks' ) 6. Use spam filter on new emails in practice Here is the code for inference of the model on new emails. test_spam_list = [ \"Participate in our new lottery!\" , \"Try out this new medicine\" ] test_ham_list = [ \"See the minutes from the last meeting attached\" , \"Investors are coming to our office on Monday\" ] test_emails = [( email_content , \"spam\" ) for email_content in test_spam_list ] test_emails += [( email_content , \"ham\" ) for email_content in test_ham_list ] new_test_set = [( get_features ( email ), label ) for ( email , label ) in test_emails ] evaluate ( train_set , new_test_set , classifier ) while True : email = input ( \"Type in your email here (or press 'Enter'): \" ) if len ( email ) == 0 : break else : prediction = classifier . classify ( get_features ( email )) print ( f \"This email is likely { prediction } \\n \" )","title":"Suggested pipeline of the project"},{"location":"projects/","text":"","title":"Projects"},{"location":"projects/age_classification/","text":"Summary The aim of this project is to make a research and find a solution for age classification based on face recognition. Based on face images collected from the camera, the ML model should classify faces into one of the specific age ranges. Sources Review The following articles were analysed: Gupta, S.K., Nain, N. Review: Single attribute and multi attribute facial gender and age estimation. Multimed Tools Appl (2022) ELKarazle, K.; Raman, V.; Then, P. Facial Age Estimation Using Machine Learning Techniques: An Overview. Big Data Cogn. Comput. 2022, 6, 128. Research Results The result of facial age estimation can be either age range (classification problem) or exact age value (regression problem). Main challenges in building efficient age prediction systems are: Real-life factors of face imaging such as resolution , sharpness , illumination , expression , occlusion , profile , frontal view , constraint environment , unconstraint environment , longitudinal , race , hair , scale , etc. Datasets creation Existing datasets usually divided into the following types: controlled (prepared in controlled environment with limited variability). Examples: FG-NET , MORPH , VADANA uncontrolled (prepared in real-life with different variability). Examples: LFW , IMDB-WIKI , ChaLearn Looking at People (LAP) , Specs on Face (SoF) , MSU LFW+ , YGA Steps for dataset creation: Image of faces collection Pre-processing cropping rotating aligning augmenting Evaluation metrics used: accuracy : \\((true positive + true negative)/(total test samples)\\) , and Mean Absolute Error (MAE) : mean value of the absolute differences between predicted age and real age (ground truth) of test samples Steps of model building 1. Feature extraction Extract unique and distinguishable patterns related to particular age classes. That includes facial age features such as texture or edge relathioships of facial skin. Global Features - apperance based feature extraction. The whole face is considered as feature space. Local Features - geometry based feature extraction. Feature extraction from facial parts as eyebrow, nose, lips, etc. 2. Building classification or regression model Handcrafted-based approach based on combination of filters: Histogram of Oriented Gradients (HOG) or Local Binary Pattern (LBP) to extract edges and shapes from facial image Deep learning approach based on CNNs. Examples of pre-trained models used for age estimation: VGG-16, VGG-19, ResNet50, AlexNet, Xception, GoogleLeNet, MobileNetV2","title":"Age Classification"},{"location":"projects/age_classification/#summary","text":"The aim of this project is to make a research and find a solution for age classification based on face recognition. Based on face images collected from the camera, the ML model should classify faces into one of the specific age ranges.","title":"Summary"},{"location":"projects/age_classification/#sources-review","text":"The following articles were analysed: Gupta, S.K., Nain, N. Review: Single attribute and multi attribute facial gender and age estimation. Multimed Tools Appl (2022) ELKarazle, K.; Raman, V.; Then, P. Facial Age Estimation Using Machine Learning Techniques: An Overview. Big Data Cogn. Comput. 2022, 6, 128.","title":"Sources Review"},{"location":"projects/age_classification/#research-results","text":"The result of facial age estimation can be either age range (classification problem) or exact age value (regression problem). Main challenges in building efficient age prediction systems are: Real-life factors of face imaging such as resolution , sharpness , illumination , expression , occlusion , profile , frontal view , constraint environment , unconstraint environment , longitudinal , race , hair , scale , etc. Datasets creation Existing datasets usually divided into the following types: controlled (prepared in controlled environment with limited variability). Examples: FG-NET , MORPH , VADANA uncontrolled (prepared in real-life with different variability). Examples: LFW , IMDB-WIKI , ChaLearn Looking at People (LAP) , Specs on Face (SoF) , MSU LFW+ , YGA Steps for dataset creation: Image of faces collection Pre-processing cropping rotating aligning augmenting Evaluation metrics used: accuracy : \\((true positive + true negative)/(total test samples)\\) , and Mean Absolute Error (MAE) : mean value of the absolute differences between predicted age and real age (ground truth) of test samples","title":"Research Results"},{"location":"projects/age_classification/#steps-of-model-building","text":"1. Feature extraction Extract unique and distinguishable patterns related to particular age classes. That includes facial age features such as texture or edge relathioships of facial skin. Global Features - apperance based feature extraction. The whole face is considered as feature space. Local Features - geometry based feature extraction. Feature extraction from facial parts as eyebrow, nose, lips, etc. 2. Building classification or regression model Handcrafted-based approach based on combination of filters: Histogram of Oriented Gradients (HOG) or Local Binary Pattern (LBP) to extract edges and shapes from facial image Deep learning approach based on CNNs. Examples of pre-trained models used for age estimation: VGG-16, VGG-19, ResNet50, AlexNet, Xception, GoogleLeNet, MobileNetV2","title":"Steps of model building"},{"location":"projects/book_summarizer/","text":"","title":"Book Summarizer"},{"location":"software_architecture/","text":"Software Architecture This section contains knowledge about software architecture concepts, patterns, and best practices. Due to my love of Python, architectural examples are provided in Python. Sources Overview Books Software Architecture for Busy Developers YouTube Videos Architecture of modern WEB applications. Evolution from A to Z Courses Educative.io Main Concepts Architecture - set of modules and components of the system, description of how these modules and components should be developed, how they should be connected, and interfaces that specify the purpose of each module and component. Components inside modules should be tightly-coupled . Modules should be loosely-coupled . Deletion/revision of a module should be simple, and should not affect other modules significantly. Separation of concerns - different parts of a system should manage different parts of the process. Inversion of control - helps to avoid strong coupling between components of the system. It realised by wrapping components and expose specific interface. Thus, components can be connected via interface rather than their hard-coded specific implementation. Think of the following responsibilities while architecting software: Address functional and non-functional requirements Use technical standards, coding best practices and design patterns Interact with stakeholders to ensure smooth development and implementation Play an active role in the development process Proactive watching over tech trends and paradigm shifts, but do not follow them blindly Main architectural principles DRY SOLID KISS PATTERNS Architecture of WEB Application Each component of the above schema is described in the respective topic of the Backend or Frontend sections. Architectural Styles Monolith benefits: easy to develop easy to deploy (as a single package) easy to monitor The key is simplicity, as long as the application remains lightweight. challenges: adding new features and debugging becomes complicated while the app grows the code is tightly coupled, so it violates single responsibility principle lack of granular scalability availability at risk (error in one module can cause of cascade blocking of the whole app) high risk of technical debt Service Oriented Architecture (SOA) It has middle core componet - Enterprise Service Bus benefits: decouple application and services better governance, as SOA aims to provide single source of truth reusability of components technology agnostic scalability, as each service is independent challenges: lack of agility","title":"Software Architecture"},{"location":"software_architecture/#software-architecture","text":"This section contains knowledge about software architecture concepts, patterns, and best practices. Due to my love of Python, architectural examples are provided in Python.","title":"Software Architecture"},{"location":"software_architecture/#sources-overview","text":"","title":"Sources Overview"},{"location":"software_architecture/#books","text":"Software Architecture for Busy Developers","title":"Books"},{"location":"software_architecture/#youtube-videos","text":"Architecture of modern WEB applications. Evolution from A to Z","title":"YouTube Videos"},{"location":"software_architecture/#courses","text":"Educative.io","title":"Courses"},{"location":"software_architecture/#main-concepts","text":"Architecture - set of modules and components of the system, description of how these modules and components should be developed, how they should be connected, and interfaces that specify the purpose of each module and component. Components inside modules should be tightly-coupled . Modules should be loosely-coupled . Deletion/revision of a module should be simple, and should not affect other modules significantly. Separation of concerns - different parts of a system should manage different parts of the process. Inversion of control - helps to avoid strong coupling between components of the system. It realised by wrapping components and expose specific interface. Thus, components can be connected via interface rather than their hard-coded specific implementation. Think of the following responsibilities while architecting software: Address functional and non-functional requirements Use technical standards, coding best practices and design patterns Interact with stakeholders to ensure smooth development and implementation Play an active role in the development process Proactive watching over tech trends and paradigm shifts, but do not follow them blindly Main architectural principles DRY SOLID KISS PATTERNS","title":"Main Concepts"},{"location":"software_architecture/#architecture-of-web-application","text":"Each component of the above schema is described in the respective topic of the Backend or Frontend sections.","title":"Architecture of WEB Application"},{"location":"software_architecture/#architectural-styles","text":"Monolith benefits: easy to develop easy to deploy (as a single package) easy to monitor The key is simplicity, as long as the application remains lightweight. challenges: adding new features and debugging becomes complicated while the app grows the code is tightly coupled, so it violates single responsibility principle lack of granular scalability availability at risk (error in one module can cause of cascade blocking of the whole app) high risk of technical debt Service Oriented Architecture (SOA) It has middle core componet - Enterprise Service Bus benefits: decouple application and services better governance, as SOA aims to provide single source of truth reusability of components technology agnostic scalability, as each service is independent challenges: lack of agility","title":"Architectural Styles"},{"location":"software_architecture/architectural_patterns/mv_patterns/","text":"MV* Architectural Patterns Brief description of the following patterns: MVC MVVM MVP Key Ideas Split business logic of the application from interface. Business logic is the core functionality that solves specific business problem ( for example, user registration, payments, add item to the basket, blog post creation, etc.) It is an algorithm of process that we want to implement to transform data somehow in order to provide a service (solve business problem). Sometimes it is called \"use case\". Interface is the graphical interface that user interacts with. Such as forms, buttons, etc. Controller is an intermediary between the UI (View) and business logic (Model). It should be 'thin', so it should not duplicate business logic or take any part of it, only some 'light' features, such as values validation.","title":"Architectural Patterns"},{"location":"software_architecture/architectural_patterns/mv_patterns/#mv-architectural-patterns","text":"Brief description of the following patterns: MVC MVVM MVP","title":"MV* Architectural Patterns"},{"location":"software_architecture/architectural_patterns/mv_patterns/#key-ideas","text":"Split business logic of the application from interface. Business logic is the core functionality that solves specific business problem ( for example, user registration, payments, add item to the basket, blog post creation, etc.) It is an algorithm of process that we want to implement to transform data somehow in order to provide a service (solve business problem). Sometimes it is called \"use case\". Interface is the graphical interface that user interacts with. Such as forms, buttons, etc. Controller is an intermediary between the UI (View) and business logic (Model). It should be 'thin', so it should not duplicate business logic or take any part of it, only some 'light' features, such as values validation.","title":"Key Ideas"}]}